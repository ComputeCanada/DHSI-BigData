{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The goal of this exercise is to go through a whole workflow of an example big data DH project: scraping the web for data, downloading the data, putting the data in appropriate storage, analysing it, and presenting the results in human accessible format.  In this example, the data will be a set of old photographs, and the analysis will consist of trying to identify images which contain objects with text in them, for example advertising billboards and street signs.\n",
    "\n",
    "You can work through the exercise at your own pace.  The instructors are available to help you and will answer your questions.  If any of the steps are beyond your current capabilities, please let the instructors know and we will provide tools which can help you proceed further.\n",
    "\n",
    "# Dataset\n",
    "\n",
    "The dataset we will look at will be the Farm Security Administration/Office of War Information (FSA-OWI) photographs archived on the Library of Congress website (http://www.loc.gov/pictures/collection/fsa/). The photographs were taken in the 1930s and 1940s, during the Great Depression and World War II.  This massive collection contains over 170,000 images, documenting all aspects of life in the United States in those years.  One of the most iconic photographs taken during the Depression , [\"Migrant Mother\"](http://www.loc.gov/pictures/collection/fsa/item/fsa1998021539/PP/) is a part of this collection.\n",
    "\n",
    "One of the key benefits of using this dataset is that all the photographs are in the Public Domain, since they were made by employees of the US government.\n",
    "\n",
    "However, the documentation of the collection is incomplete .  69,000 of the photographs are untitled, and for some of these no information at all is available.  Even for photographs containing accopanying information, the title and caption might not fully indicate the interesting aspects of a photograph.  The goal for this exercise is to analyse the untitled photographs in the collection and identify those which contain text.  The text can then be used to deduce more information about the photograph.\n",
    "\n",
    "# Scraping the web\n",
    "\n",
    "The Library of Congress website provides a search interface to its collection.  However, this search interface has not been updated for over a decade and, while very useful, does not provide all the functionality that a researcher might need.  For example, while the search can be limited to return only images which are digitized, it cannot be set to return only images which have been digitized in high resolution.  The images available only in low resolution  which are returned are usually not useful for analysis, and we want to modify our search process so that they are discarded.\n",
    "Also, the search results are only available in a webpage, and cannot be easily downloaded to a convenient dataset for further processing, so we need to write a program which can collect them to a convenient format for future use.\n",
    "\n",
    "An example program for scraping the web which you can build on is below. You should write a program that scans over successive pages of a search for a term, (\"untitled\" in this case)\n",
    "\n",
    "`http://www.loc.gov/pictures/search/?q=untitled&fa=displayed%3Aanywhere&sp=1&co=fsa&st=grid`\n",
    "\n",
    "`http://www.loc.gov/pictures/search/?q=untitled&fa=displayed%3Aanywhere&sp=2&co=fsa&st=grid`\n",
    "\n",
    "`...`\n",
    "\n",
    "`http://www.loc.gov/pictures/search/?q=untitled&fa=displayed%3Aanywhere&sp=696&co=fsa&st=grid`\n",
    "\n",
    "and extracts the addresses of each of the 100 images in each page, for example:\n",
    "\n",
    "`http://www.loc.gov/pictures/collection/fsa/item/fsa1997000003/PP/`\n",
    "\n",
    "For each of these pages, you should analyze whether a high resolution TIFF version of the image is available. If it is, you want to save the image link and title.\n",
    "\n",
    "The natural way to store this information would be a database.  A Python example with the details of setting up and using a simple database is provided below.  If you need more information on how to work with sqlite database, you can find on [this page](http://www.tutorialspoint.com/sqlite/index.htm).\n",
    "\n",
    "The HTML code of any web page can be extracted using urllib module of Python.  An example of using it is provided below, with parsing via BeautifulSoup.  You can search the HTML text extracted using the string.find method, or regular expressions.\n",
    "\n",
    "# Downloading and storing the data\n",
    "\n",
    "The scraping of the web should be separated from the downloading stage since the downloads may take a long time, and the downloading program may have to be run intermittently over an extended time period.\n",
    "\n",
    "Take the existing database and add a field for indicating download status.  Then write a program which will download the files which have not yet been downloaded, and update the database as they come in.  You may want to apply a tranformation to .tiff files to reduce their size, converting them to .jpg format.  You may also want to rescale the resolution.  The imagemagick command line program is the most convenient tool for this.\n",
    "\n",
    "Once the program is done, set it running to download a set of files.  If that is too time consuming, a previously prepared set of images is also available (ask instructor). \n",
    "\n",
    "# Detecting text in image\n",
    "Detecting text in photographic images (as opposed to scans of pages with text) is still a developing field, and the problem is rather challenging.  The difficulty often lies in detecting that text is present somewhere in the image in the first place.\n",
    "\n",
    "For this exercise we will use a standard OCR tool called Tesseract. This software is conveniently available in all commonly used distributions of Linux.  Applying it to photographs is not its typical use, but it works remarkably well for this exercise.  In any future project one would use better tools as they become available.\n",
    "\n",
    "First, try to detect text in a single [image](http://loc.gov/pictures/resource/fsa.8a04355/) (first download the [high resolution .tif version of the image](http://cdn.loc.gov/master/pnp/fsa/8a04000/8a04300/8a04355a.tif) ), for example:\n",
    "\n",
    "`tesseract 8a04355a.tif out.txt`\n",
    "\n",
    "The text recognized in the image is very clear and the program does a relatively good job in recognising it and producing understandable output, stored in file out.txt.\n",
    "\n",
    "Then try a more challenging [image](http://www.loc.gov/pictures/collection/fsa/item/fsa1997003919/PP/). Here no text is detected.  Try rotating the image slightly with imagemagick and see if detection improves.\n",
    "\n",
    " `convert 8a03931u.tif  -rotate 10 output_rotated10.JPG`\n",
    "\n",
    " `tesseract output_rotated10.JPG out.txt`\n",
    " \n",
    " At the end, write a script which tries to reliably detects text in image. You might want to try a few rotations of the image in your attempts.  Your script should count the characters in the output: once some threshold is exceeded, you can have some confidence that the photograph contains some text.\n",
    "\n",
    "# Analysing the data\n",
    "Write a Spark script which will apply the image analysis script to all the files in your data set.  You could add a field to the database indicate whether text has been detected in the image or not.  Run the script on the set of images you gathered to detect those which contain text.\n",
    "\n",
    "# Presenting the data\n",
    "\n",
    "Organize the output in some useful way so that humans can easily scan through it.  Jupyter Notebook can be used to do that.  One could also make a webpage with images containing text embedded.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Useful code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get HTML source code of webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def getpage(urladdress):\n",
    "    return requests.get(urladdress).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_source_code = getpage(\"http://www.dhsi.org/\")\n",
    "soup = BeautifulSoup(html_source_code, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download image from web\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(BytesIO(requests.get(\"https://pbs.twimg.com/media/CkxPtG6UYAEXH8r.jpg\").content))\n",
    "img.save('twitter_deer.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')[4].attrs['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlite3 database in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "data_file='mydatabase.db'\n",
    "\n",
    "if(os.path.isfile(data_file) ):\n",
    "    print(\"found existing data file\")\n",
    "    conn = sqlite3.connect(data_file)\n",
    "    c = conn.cursor()\n",
    "else:\n",
    "    print(\"creating new database file\")\n",
    "    conn = sqlite3.connect(data_file)\n",
    "    c = conn.cursor()\n",
    "    # create table in database\n",
    "    c.execute(\"CREATE TABLE mytable (birthyear int, first_name text, last_name text)\")\n",
    "    conn.commit()\n",
    "    \n",
    "# enter data into table created\n",
    "update=(1812,\"Charles\",\"Dickens\")\n",
    "print(update)\n",
    "c.execute(\"INSERT INTO mytable VALUES (?,?,?)\",update)\n",
    "conn.commit()\n",
    "\n",
    "# extract and modify data from table\n",
    "c.execute(\"SELECT * from mytable\")\n",
    "y=c.fetchall()\n",
    "for row in y:\n",
    "    print(y)\n",
    "    new_name=\"Charles John Huffam\"\n",
    "    t=[new_name,]\n",
    "    c.execute(\"UPDATE mytable SET first_name=? where last_name=\\\"Dickens\\\"\",t)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Workshop collective solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpage(urladdress):\n",
    "    return requests.get(urladdress).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_source_code = getpage('http://www.loc.gov/pictures/search/?q=untitled&fa=displayed%3Aanywhere&sp=1&co=fsa&st=grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_source_code, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_preview_link(link_object):\n",
    "    return 'preview' in link_object.attrs.get('class', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_preview_link(all_links[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_preview_link(all_links[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_links = list(filter(keep_preview_link, all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_links_text = list(map(lambda x: 'http:' + x.attrs['href'], preview_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_link_rdd = sc.parallelize(preview_links_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page_rdd = preview_link_rdd.map(lambda x : (x, getpage(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jpg_image_url(html):\n",
    "    links = BeautifulSoup(html, \"lxml\").find_all('link')\n",
    "    tifs = []    \n",
    "    for link in links:\n",
    "        if link.attrs.get('type', None) == 'image/jpg':\n",
    "            tifs.append('http:' + link.attrs['href'])\n",
    "    return tifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_link = html_page_rdd.mapValues(extract_jpg_image_url).mapValues(lambda link_list: link_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://www.loc.gov/pictures/collection/fsa/item/2017713829/',\n",
       " 'http://cdn.loc.gov/service/pnp/fsa/8a00000/8a00000/8a00016r.jpg')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photo_link.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "def getimage(link):\n",
    "    img_bytes = requests.get(link).content\n",
    "    return Image.open(BytesIO(img_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_rdd = photo_link.mapValues(getimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do some object recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Collecting google-cloud-vision\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/53/2c98885401a959b63c1a69537f1b5169d73e2df0bd86591dd1e8611b1302/google_cloud_vision-0.32.0-py2.py3-none-any.whl (108kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 828kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=0.1.0 (from google-cloud-vision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/03/a83c6d0efa63a13d085b81927fdc9e12ffb98aa0f67798a7573fc6b231e2/google_api_core-1.2.1-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 490kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /dev/shm/user10/venv/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (1.11.0)\n",
      "Collecting protobuf>=3.0.0 (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/f8/d09e4bf21c4de65405ce053e90542e728c5b7cf296b9df36b0bf0488f534/protobuf-3.6.0-py2.py3-none-any.whl (390kB)\n",
      "\u001b[K    100% |████████████████████████████████| 399kB 602kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=34.0.0 in /dev/shm/user10/venv/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (39.2.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /dev/shm/user10/venv/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (2.18.4)\n",
      "Collecting google-auth<2.0.0dev,>=0.4.0 (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/06/6e6d5bfa4d23ee40efd772d6b681a7afecd859a9176e564b8c329382370f/google_auth-1.5.0-py2.py3-none-any.whl (65kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 421kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /dev/shm/user10/venv/lib/python3.6/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (2018.4)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.5.3 (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/03/d25bed04ec8d930bcfa488ba81a2ecbf7eb36ae3ffd7e8f5be0d036a89c9/googleapis-common-protos-1.5.3.tar.gz\n",
      "Collecting grpcio>=1.8.2; extra == \"grpc\" (from google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision) (2018.4.16)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/51/bcd96bf6231d4b2cc5e023c511bee86637ba375c44a6f9d1b4b7ad1ce4b9/pyasn1_modules-0.2.1-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 484kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa>=3.1.4 (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "Collecting cachetools>=2.0.0 (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/58/cbee863250b31d80f47401d04f34038db6766f95dea1cc909ea099c7e571/cachetools-2.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.1 (from pyasn1-modules>=0.2.1->google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.0->google-cloud-vision)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 604kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: googleapis-common-protos\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/user10/.cache/pip/wheels/62/45/af/649bbf07b6595fda010be1bda667cd56d0444d07afc6f8b687\n",
      "Successfully built googleapis-common-protos\n",
      "Installing collected packages: protobuf, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, googleapis-common-protos, grpcio, google-api-core, google-cloud-vision\n",
      "Successfully installed cachetools-2.1.0 google-api-core-1.2.1 google-auth-1.5.0 google-cloud-vision-0.32.0 googleapis-common-protos-1.5.3 grpcio-1.12.1 protobuf-3.6.0 pyasn1-0.4.3 pyasn1-modules-0.2.1 rsa-3.4.2\n"
     ]
    }
   ],
   "source": [
    "! pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to https://console.cloud.google.com/apis/credentials/serviceaccountkey\n",
    "- From the Service account drop-down list, select New service account.\n",
    "- Enter a name into the Service account name field.\n",
    "- Don't select a value from the Role drop-down list. No role is required to access this service.\n",
    "- Click Create. A note appears, warning that this service account has no role.\n",
    "- Click Create without role. A JSON file that contains your key downloads to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.cloud.vision import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_rdd = photo_link.mapValues(lambda url : requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image_content(img_bytes):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/user10/DHSI-BigData/Day_4_Exercise/gcreds.json'    \n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    image = types.Image(content=img_bytes_1)\n",
    "    response = client.web_detection(image=image)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_entities_rdd = bytes_rdd.mapValues(detect_image_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
