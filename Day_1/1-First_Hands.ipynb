{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Interactive Big Data Analysis with Spark\n",
    "<img src=\"http://www.copyrightuser.org/wp-content/uploads/2017/05/text_data_mining.jpg\">\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Curation]()\n",
    "2. [Preparation]()\n",
    "  1. [Data Importation](#2.A-Data-Importation)\n",
    "  2. [Package Installation](#2.B-Package-Installation)\n",
    "  3. [Package Importation](#2.C-Package-Importation)\n",
    "  4. [Context Creation](#2.D-Context-Creation)\n",
    "3. [Preprocessing]()\n",
    "  1. [Creating an RDD](#3.A-Creating-an-RDD)\n",
    "  2. [Getting Help](#3.B-Getting-Help)\n",
    "  3. [Action on a Dataset](#4.-Action-on-a-Dataset)\n",
    "  4. [Dataset Transformation](#5.-Dataset-Transformation)\n",
    "  5. [Filtering a Dataset](#7.-Filtering-a-Dataset)\n",
    "  6. [Caching a Dataset](#6.-Caching-a-Dataset)\n",
    "4. [Processing]()\n",
    "  1. [Valorizing data by transforming the dataset](#4.A-Valorizing-data-by-transforming-the-dataset)\n",
    "  2. [First analysis: authors' life expectancy](#4.B-First-analysis:-authors'-life-expectancy)\n",
    "  3. [Second analysis: authors' life expectancy... done correctly](#4.C-Second-analysis:-authors'-life-expectancy...-done-correctly)\n",
    "5. [Storage]()\n",
    "6. [Applying new knowledge](#6.-Applying-new-knowledge)\n",
    "  1. [Preprocessing the pages to extract the text](#6.A-Preprocessing-the-pages-to-extract-the-text)\n",
    "  2. [Processing: Analysing the work of an era](#6.B-Processing:-Analysing-the-work-of-an-era)\n",
    "  3. [Learning: Learning: Topic modelling](#6.C-Learning:-Topic-modelling)\n",
    "\n",
    "## List of Exercises\n",
    "1. [Exercise 1: How to RDD?](#Exercise-1:-How-to-RDD?)\n",
    "2. [Exercise 2: How to Count?](#Exercise-2:-How-to-Count?)\n",
    "3. [Exercise 3: How to Transform?](#Exercise-3:-How-to-Transform?)\n",
    "4. [Exercise 4: How to Filter?](#Exercise-4:-How-to-Filter?)\n",
    "5. [Exercise 5: How to Extract?](#Exercise-5:-How-to-Extract?)\n",
    "6. [Exercise 6: How to Reduce?](#Exercise-6:-How-to-Reduce?)\n",
    "7. [Exercise 7: How to Cache?](#Exercise-7:-How-to-Cache?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Curation\n",
    "\n",
    "<img src=\"https://ucarecdn.com/d6f7ca55-9121-4d29-a5b3-7bf165b2c9bf/\">\n",
    "\n",
    "From Wikipedia:\n",
    "> Data curation is a term used to indicate management activities related to organization and integration of data collected from various sources, annotation of the data, and publication and presentation of the data such that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes \"all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data\". In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database. The term is also used in the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation. In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component.\n",
    "\n",
    "> According to the University of Illinois' Graduate School of Library and Information Science, \"Data curation is the active and on-going management of data through its lifecycle of interest and usefulness to scholarship, science, and education; curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time.\"\n",
    "\n",
    "Curation being a field of its own, we will pass on the actual technics behind it. For this course, we will use a precurated dataset, the [eBooks@Adelaide dataset](https://ebooks.adelaide.edu.au/).\n",
    "\n",
    "The data we will use has been scraped from the website and converted into [JSON](https://en.wikipedia.org/wiki/JSON) file.\n",
    "<img src=\"http://prowebscraping.com/wp-content/uploads/2015/09/web-scraping-process1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-oWruWThh0Vo/UtbO00mqpnI/AAAAAAAAAaU/zRZdUBY1I14/s1600/png;base646cf98dd61304919c.png\" width=\"50%\">\n",
    "\n",
    "### 2.A Data Importation\n",
    "In order for all nodes of our cluster to access our data, we have previously imported the data in [NFS](https://en.wikipedia.org/wiki/Network_File_System). Here are the commands that we could have used to import the data in [HDFS](https://en.wikipedia.org/wiki/Apache_Hadoop#HDFS) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Shell\n",
    "hdfs dfs -mkdir /adelaide/\n",
    "hdfs dfs -mkdir /adelaide/meta\n",
    "hdfs dfs -mkdir /adelaide/page\n",
    "hdfs dfs -put ~/datasets/meta/*.json /adelaide/meta\n",
    "hdfs dfs -put ~/datasets/meta/*.json /adelaide/meta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the data is actually available by listing the content of the dataset folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adelaide_meta_A.json  adelaide_meta_J.json  adelaide_meta_S.json\r\n",
      "adelaide_meta_B.json  adelaide_meta_K.json  adelaide_meta_T.json\r\n",
      "adelaide_meta_C.json  adelaide_meta_L.json  adelaide_meta_U.json\r\n",
      "adelaide_meta_D.json  adelaide_meta_M.json  adelaide_meta_V.json\r\n",
      "adelaide_meta_E.json  adelaide_meta_N.json  adelaide_meta_W.json\r\n",
      "adelaide_meta_F.json  adelaide_meta_O.json  adelaide_meta_X.json\r\n",
      "adelaide_meta_G.json  adelaide_meta_P.json  adelaide_meta_Y.json\r\n",
      "adelaide_meta_H.json  adelaide_meta_Q.json  adelaide_meta_Z.json\r\n",
      "adelaide_meta_I.json  adelaide_meta_R.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls /project/datasets/adelaide/meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adelaide_page_A.json  adelaide_page_J.json  adelaide_page_S.json\r\n",
      "adelaide_page_B.json  adelaide_page_K.json  adelaide_page_T.json\r\n",
      "adelaide_page_C.json  adelaide_page_L.json  adelaide_page_U.json\r\n",
      "adelaide_page_D.json  adelaide_page_M.json  adelaide_page_V.json\r\n",
      "adelaide_page_E.json  adelaide_page_N.json  adelaide_page_W.json\r\n",
      "adelaide_page_F.json  adelaide_page_O.json  adelaide_page_X.json\r\n",
      "adelaide_page_G.json  adelaide_page_P.json  adelaide_page_Y.json\r\n",
      "adelaide_page_H.json  adelaide_page_Q.json  adelaide_page_Z.json\r\n",
      "adelaide_page_I.json  adelaide_page_R.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls /project/datasets/adelaide/page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Python Package Installation\n",
    "\n",
    "To analyze our data, we will need some Python packages:  \n",
    "- numpy for numeric data manipulation;\n",
    "- networkx for network analysis;\n",
    "- plotly for plotting;\n",
    "- beautifulsoup4 to parse and extract data from HTML pages.\n",
    "\n",
    "These packages can be installed with the following command:\n",
    "```\n",
    "pip install numpy networkx plotly beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/avx2, /cvmfs/soft.computecanada.ca/custom/python/wheelhouse/generic\n",
      "Collecting numpy\n",
      "Collecting networkx\n",
      "Collecting plotly\n",
      "Collecting beautifulsoup4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.2MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.1.0 in /dev/shm/user10/venv/lib/python3.6/site-packages (from networkx) (4.3.0)\n",
      "Collecting pytz (from plotly)\n",
      "Requirement already satisfied: nbformat>=4.2 in /dev/shm/user10/venv/lib/python3.6/site-packages (from plotly) (4.4.0)\n",
      "Requirement already satisfied: requests in /dev/shm/user10/venv/lib/python3.6/site-packages (from plotly) (2.18.4)\n",
      "Requirement already satisfied: six in /dev/shm/user10/venv/lib/python3.6/site-packages (from plotly) (1.11.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /dev/shm/user10/venv/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.3.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /dev/shm/user10/venv/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (2.6.0)\n",
      "Requirement already satisfied: jupyter-core in /dev/shm/user10/venv/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.4.0)\n",
      "Requirement already satisfied: ipython-genutils in /dev/shm/user10/venv/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (0.2.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests->plotly) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests->plotly) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests->plotly) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /dev/shm/user10/venv/lib/python3.6/site-packages (from requests->plotly) (2018.4.16)\n",
      "Installing collected packages: numpy, networkx, pytz, plotly, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.6.0 networkx-2.1 numpy-1.14.3 plotly-2.7.0 pytz-2018.4\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy networkx plotly beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C Package Importation\n",
    "\n",
    "In this notebook, we will use [Apache Spark](http://spark.apache.org) to analyze briefly the Adelaide University's Book Dataset.\n",
    "\n",
    "First, we need to import Spark's Python module named `pyspark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import some Python standard modules that will help us during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we import an interactive chart draing library [plotly](https://plot.ly/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly import graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D Context Creation\n",
    "\n",
    "Once we have imported the required packages, we need to create a SparkContext. The context is an object that allows us to interact with the Spark cluster and create new resilient distributed dataset (RDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName(\"AdelaideNotebook\")\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except:\n",
    "    print(\"Warning : a SparkContext already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context reads Spark configuration files and automatically deduces the configuration of our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "<img src=\"http://adcieo.com/wp-content/uploads/2015/02/cleansing.jpg\">\n",
    "### 3.A Creating an RDD\n",
    "\n",
    "We will now create an RDD. The RDD will be created by reading JSON files containing the books' informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_json = sc.textFile('/project/datasets/adelaide/meta/*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an entry of the `adelaide_meta_json` RDD:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"@context\": \"http://schema.org\", \n",
    "  \"dateModified\": \"2014-02-26\", \n",
    "  \"image\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/cover.jpg\", \n",
    "  \"author\": \"Bowen, Marjorie, 1885-1952\", \n",
    "  \"@type\": \"Book\", \n",
    "  \"source\": \"https://gutenberg.net.au/ebooks09/0900581.txt\", \n",
    "  \"inLanguage\": \"en\", \n",
    "  \"publisher\": \"The University of Adelaide Library\", \n",
    "  \"name\": \"The Avenging of Ann Leete\", \n",
    "  \"keywords\": \"Literature\", \n",
    "  \"url\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/\", \n",
    "  \"description\": \"The Avenging of Ann Leete / Marjorie Bowen\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a few entries with the RDD's method `take` to get the first `K` elements of the meta information dataset. Here, `K = 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"url\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"1844\", \"datePublished\": \"2015-10-26\", \"@type\": \"Book\", \"author\": \"Emerson, Ralph Waldo, 1803-1882\", \"name\": \"New England Reformers\", \"description\": \"New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson\", \"image\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}', '{\"url\": \"https://ebooks.adelaide.edu.au/m/maupassant/guy/new-sensation/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"\", \"datePublished\": \"2016-01-26\", \"@type\": \"Book\", \"author\": \"Maupassant, Guy de, 1850-1893\", \"name\": \"The New Sensation\", \"description\": \"The New Sensation : (Parisine) [] / Guy de Maupassant\", \"image\": \"https://ebooks.adelaide.edu.au/m/maupassant/guy/new-sensation/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}', '{\"author\": \"Lawrence, D. H. (David Herbert), 1885-1930\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"inLanguage\": \"en\", \"dateCreated\": \"1934\", \"name\": \"New Eve and Old Adam\", \"@type\": \"Book\", \"url\": \"https://ebooks.adelaide.edu.au/l/lawrence/dh/new-eve-and-old-adam/\", \"image\": \"https://ebooks.adelaide.edu.au/l/lawrence/dh/new-eve-and-old-adam/cover.jpg\", \"description\": \"New Eve and Old Adam / D. H. Lawrence\", \"publisher\": \"The University of Adelaide Library\"}', '{\"url\": \"https://ebooks.adelaide.edu.au/h/hume/david/h92n/\", \"@context\": \"http://schema.org\", \"contributors\": [\"Robertson, John Mackinnon, 1856-1933\", \"Hume, David, 1711-1776\"], \"dateCreated\": \"1757\", \"datePublished\": \"2009-12-09\", \"dateModified\": \"2014-02-26\", \"@type\": \"Book\", \"author\": \"Hume, David, 1711-1776\", \"name\": \"The Natural History of Religion\", \"keywords\": \"Philosophy\", \"description\": \"The Natural History of Religion / David Hume; with an introduction by John M. Robertson\", \"image\": \"https://ebooks.adelaide.edu.au/h/hume/david/h92n/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}']\n"
     ]
    }
   ],
   "source": [
    "meta_first4 = adelaide_meta_json.take(4)\n",
    "print(meta_first4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `take` returns a list, we can iterate on the result and each book information on a seperate line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"url\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"1844\", \"datePublished\": \"2015-10-26\", \"@type\": \"Book\", \"author\": \"Emerson, Ralph Waldo, 1803-1882\", \"name\": \"New England Reformers\", \"description\": \"New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson\", \"image\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}\n",
      "{\"url\": \"https://ebooks.adelaide.edu.au/m/maupassant/guy/new-sensation/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"\", \"datePublished\": \"2016-01-26\", \"@type\": \"Book\", \"author\": \"Maupassant, Guy de, 1850-1893\", \"name\": \"The New Sensation\", \"description\": \"The New Sensation : (Parisine) [] / Guy de Maupassant\", \"image\": \"https://ebooks.adelaide.edu.au/m/maupassant/guy/new-sensation/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}\n",
      "{\"author\": \"Lawrence, D. H. (David Herbert), 1885-1930\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"inLanguage\": \"en\", \"dateCreated\": \"1934\", \"name\": \"New Eve and Old Adam\", \"@type\": \"Book\", \"url\": \"https://ebooks.adelaide.edu.au/l/lawrence/dh/new-eve-and-old-adam/\", \"image\": \"https://ebooks.adelaide.edu.au/l/lawrence/dh/new-eve-and-old-adam/cover.jpg\", \"description\": \"New Eve and Old Adam / D. H. Lawrence\", \"publisher\": \"The University of Adelaide Library\"}\n",
      "{\"url\": \"https://ebooks.adelaide.edu.au/h/hume/david/h92n/\", \"@context\": \"http://schema.org\", \"contributors\": [\"Robertson, John Mackinnon, 1856-1933\", \"Hume, David, 1711-1776\"], \"dateCreated\": \"1757\", \"datePublished\": \"2009-12-09\", \"dateModified\": \"2014-02-26\", \"@type\": \"Book\", \"author\": \"Hume, David, 1711-1776\", \"name\": \"The Natural History of Religion\", \"keywords\": \"Philosophy\", \"description\": \"The Natural History of Religion / David Hume; with an introduction by John M. Robertson\", \"image\": \"https://ebooks.adelaide.edu.au/h/hume/david/h92n/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}\n"
     ]
    }
   ],
   "source": [
    "for entry in meta_first4:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: How to RDD?\n",
    "\n",
    "Create a new RDD named `adelaide_page_json` that contains the book URLs and its content as HTML code.\n",
    "\n",
    "The path to the page files is `/project/datasets/adelaide/page/`.\n",
    "\n",
    "Here is an example of an entry of the `adelaide_page_json` RDD:\n",
    "```\n",
    "[\"https://ebooks.adelaide.edu.au/m/maupassant/guy/kiss/\", \"<!DOCTYPE html>\\n\\n<html> [...]\"]\n",
    "```\n",
    "\n",
    "Then retrieve the first element of that RDD and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_json = sc.textFile('/project/datasets/adelaide/page/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/\", \"<!DOCTYPE html>\\\\n\\\\n<html xmlns=\\\\\"http://www.w3.org/1999/xhtml\\\\\">\\\\n<head>\\\\n<meta charset=\\\\\"utf-8\\\\\"/>\\\\n<title>The Accent / Guy de Maupassant</title><script type=\\\\\"application/ld+json\\\\\">\\\\n{\\\\n   \\\\\"@context\\\\\" : \\\\\"http://schema.org\\\\\",\\\\n   \\\\\"@type\\\\\" : \\\\\"Book\\\\\",\\\\n   \\\\\"author\\\\\" : \\\\\"Maupassant, Guy de, 1850-1893\\\\\",\\\\n   \\\\\"image\\\\\" : \\\\\"https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/cover.jpg\\\\\",\\\\n   \\\\\"dateCreated\\\\\" : \\\\\"\\\\\",\\\\n   \\\\\"datePublished\\\\\" : \\\\\"2016-01-26\\\\\",\\\\n   \\\\\"description\\\\\" : \\\\\"The Accent : (L\\'Accent) [] / Guy de Maupassant\\\\\",\\\\n   \\\\\"inLanguage\\\\\" : \\\\\"en\\\\\",\\\\n   \\\\\"name\\\\\" : \\\\\"The Accent\\\\\",\\\\n   \\\\\"publisher\\\\\" : \\\\\"The University of Adelaide Library\\\\\",\\\\n   \\\\\"keywords\\\\\" : \\\\\"Literature\\\\\",\\\\n   \\\\\"url\\\\\" : \\\\\"https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/\\\\\"\\\\n}\\\\n</script>\\\\n<!-- open graph -->\\\\n<meta content=\\\\\"The Accent\\\\\" property=\\\\\"og:title\\\\\"/>\\\\n<meta content=\\\\\"The Accent : (L\\'Accent) [] / Guy de Maupassant\\\\\" property=\\\\\"og:description\\\\\"/>\\\\n<meta content=\\\\\"https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/\\\\\" property=\\\\\"og:url\\\\\"/>\\\\n<meta content=\\\\\"https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/cover.jpg\\\\\" property=\\\\\"og:image\\\\\"/>\\\\n<!-- end meta -->\\\\n<link href=\\\\\"widgets/style.css\\\\\" rel=\\\\\"stylesheet\\\\\" type=\\\\\"text/css\\\\\"/><link href=\\\\\"/lib/widgets/not_epub.css\\\\\" rel=\\\\\"stylesheet\\\\\" type=\\\\\"text/css\\\\\"/>\\\\n<script type=\\\\\"text/javascript\\\\\">\\\\n\\\\n  var _gaq = _gaq || [];\\\\n  _gaq.push([\\'_setAccount\\', \\'UA-4561916-1\\']);\\\\n  _gaq.push([\\'_trackPageview\\']);\\\\n\\\\n  (function() {\\\\n    var ga = document.createElement(\\'script\\'); ga.type = \\'text/javascript\\'; ga.async = true;\\\\n    ga.src = (\\'https:\\' == document.location.protocol ? \\'https://ssl\\' : \\'http://www\\') + \\'.google-analytics.com/ga.js\\';\\\\n    var s = document.getElementsByTagName(\\'script\\')[0]; s.parentNode.insertBefore(ga, s);\\\\n  })();\\\\n\\\\n</script>\\\\n<script type=\\\\\"text/javascript\\\\\">\\\\n//<![CDATA[\\\\nfunction getKey(event) {\\\\n    k = event.keyCode;\\\\n    if (k == 37) { var h=document.querySelector(\\\\\"link[rel=prev]\\\\\").getAttribute(\\\\\"href\\\\\"); window.location=h; }\\\\n    if (k == 39) { var h=document.querySelector(\\\\\"link[rel=next]\\\\\").getAttribute(\\\\\"href\\\\\"); window.location=h; }\\\\n}\\\\ndocument.onkeydown = getKey;\\\\n//]]>\\\\n</script>\\\\n<meta content=\\\\\"width=device-width\\\\\" name=\\\\\"viewport\\\\\">\\\\n<style type=\\\\\"text/css\\\\\">\\\\n/*<![CDATA[*/\\\\n@import url(\\\\\"//fonts.googleapis.com/css?family=Old+Standard+TT:italic,bold\\\\\");\\\\n.author {font-family:\\'Old Standard TT\\', Georgia, serif;font-style:italic;font-variant:normal;font-weight:normal;}\\\\n/*]]>*/\\\\n</style>\\\\n</meta></head>\\\\n<body>\\\\n<div id=\\\\\"controls\\\\\">\\\\n<a class=\\\\\"closebtn\\\\\" href=\\\\\"#\\\\\" onclick=\\\\\"document.getElementById(\\'controls\\').style.display = \\'none\\';return false;\\\\\" title=\\\\\"Hide strip\\\\\">\\\\u2715</a>\\\\n<ul>\\\\n<li><a class=\\\\\"mdi mdi-information-outline\\\\\" href=\\\\\"/m/maupassant/guy/\\\\\" rel=\\\\\"author\\\\\" title=\\\\\"About this book\\\\\"> about</a></li>\\\\n<li><a class=\\\\\"mdi mdi-book-open\\\\\" href=\\\\\"/m/maupassant/guy/accent/\\\\\" title=\\\\\"read in browser\\\\\"> read</a></li>\\\\n<li><a class=\\\\\"mdi mdi-file\\\\\" href=\\\\\"/m/maupassant/guy/accent/\\\\\" title=\\\\\"the complete book in a single page\\\\\"> complete</a></li>\\\\n<li><a class=\\\\\"mdi mdi-download\\\\\" href=\\\\\"/cgi-bin/zip/m/maupassant/guy/accent\\\\\" title=\\\\\"Download a zip archive\\\\\"> download</a></li>\\\\n<li><a class=\\\\\"mdi mdi-cellphone-android\\\\\" href=\\\\\"/m/maupassant/guy/accent/accent.epub\\\\\" title=\\\\\"Download an ePub version\\\\\"> ePub</a></li>\\\\n<li><a class=\\\\\"mdi mdi-amazon\\\\\" href=\\\\\"/m/maupassant/guy/accent/accent.azw3\\\\\" title=\\\\\"Download a Kindle version\\\\\"> Kindle</a></li>\\\\n<li><span class=\\\\\"mdi mdi-share\\\\\"></span></li>\\\\n<li><a class=\\\\\"mdi mdi-facebook\\\\\" href=\\\\\"#\\\\\" onclick=\\\\\"window.open(\\'https://www.facebook.com/sharer/sharer.php?u=https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/\\',\\'facebookShareDialog\\',\\'width=626,height=436\\');return false;\\\\\" title=\\\\\"Share to Facebook\\\\\">\\\\u00a0</a></li>\\\\n<li><a class=\\\\\"mdi mdi-google-plus\\\\\" href=\\\\\"https://plus.google.com/share?url=https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/\\\\\" onclick=\\\\\"javascript:window.open(this.href,\\'\\',\\'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600\\');return false;\\\\\" target=\\\\\"_blank\\\\\" title=\\\\\"Share to Google Plus\\\\\">\\\\u00a0</a></li>\\\\n<li><a class=\\\\\"mdi mdi-twitter\\\\\" href=\\\\\"http://twitter.com/share?text=https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/\\\\\" onclick=\\\\\"javascript:window.open(this.href,\\'\\',\\'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=600,width=600\\');return false;\\\\\" title=\\\\\"Share to Twitter\\\\\">\\\\u00a0</a></li>\\\\n</ul></div>\\\\n<div class=\\\\\"titlepage\\\\\" id=\\\\\"titlepage\\\\\">\\\\n<div style=\\\\\"border-bottom:2px solid #00609c;margin-bottom:3em;\\\\\">\\\\n<h2><span class=\\\\\"sc\\\\\">The Short Stories<br/>\\\\nof</span><br/>\\\\n<span class=\\\\\"author fs200 lh180\\\\\">Guy de Maupassant</span>\\\\n</h2>\\\\n</div>\\\\n<h1><span class=\\\\\"title fs150\\\\\">The Accent</span><br/>\\\\n<span class=\\\\\"sc\\\\\">(L\\'Accent) []</span>\\\\n</h1>\\\\n<p class=\\\\\"imprint\\\\\">\\\\n</p>\\\\n</div>\\\\n<div class=\\\\\"titleverso\\\\\">\\\\n<p>This edition published by <a href=\\\\\"https://ebooks.adelaide.edu.au/\\\\\">eBooks@Adelaide</a>.</p>\\\\n<p>Last updated Tuesday, January 26, 2016 at 14:25.</p>\\\\n<div id=\\\\\"licence\\\\\">\\\\n<p>To the best of our knowledge, <strong>the text</strong> of this<br/>\\\\nwork is in the \\\\u201c<strong>Public Domain</strong>\\\\u201d in Australia.<br/>\\\\nHOWEVER, copyright law varies in other countries, and the work may\\\\nstill be under copyright in the country from which you are accessing this website.\\\\nIt is your responsibility to check the applicable copyright laws\\\\nin your country before downloading this work.</p>\\\\n</div>\\\\n<p>eBooks@Adelaide<br/>\\\\nThe University of Adelaide Library<br/>\\\\nUniversity of Adelaide<br/>\\\\nSouth Australia 5005</p>\\\\n</div>\\\\n<div class=\\\\\"chapter\\\\\" id=\\\\\"chapter1\\\\\">\\\\n<div class=\\\\\"header modern\\\\\">\\\\n<h3>The Accent</h3>\\\\n</div>\\\\n<p class=\\\\\"dropcap\\\\\">It was a large, upholstered house, with long white terraces shaded by vines, from which one could see the sea. Large pines stretched a dark dome over the sacked facade, and there was a look of neglect, of want and wretchedness about it all, such as irreparable losses, departures to other countries, and death leave behind them.</p>\\\\n<p>The interior wore a strange look, with half unpacked boxes serving for wardrobes, piles of band boxes, and for seats there was an array of worm-eaten armchairs, into which bits of velvet and silk, which had been cut from old dresses, had been festooned anyhow, and along the walls there were rows of rusty nails which made one think of old portraits and of pictures full of associations, which had one by one been bought for a low price by some second-hand furniture broker.</p>\\\\n<p>The rooms were in disorder and furnished no matter how, while velvets were hanging from the ceilings and in the corners, and seemed to show that as the servants were no longer paid except by hopes, they no longer did more than give them an accidental, careless touch with the broom occasionally. The drawing-room, which was extremely large, was full of useless knick-knacks, rubbish which is put up for sale at stalls at watering places, daubs, they could not be called paintings of portraits and of flowers, and an old piano with yellow keys.</p>\\\\n<p>Such is the home where she, who had been called the handsome Madame de Maurillac, was spending her monotonous existence, like some unfortunate doll which inconstant, childish hands have thrown into a corner in a loft, she who, almost passed for a professional seductress, and whose coquetries, at least so the Faithful ones of the Party said, had been able to excite a passing and last spark of desire in the dull eyes of the Emperor.</p>\\\\n<p>Like so many others, she and her husband had waited for his return from Elba, had discounted a fresh, immediate chance, had kept up boldly and spent the remains of his fortune at that game of luxury.</p>\\\\n<p>On the day when the illusion vanished, and he was forced to awake from his dream, Monsieur de Maurillac, without considering that he was leaving his wife and daughter behind him almost penniless, but not being able to make up his mind to come down in the world, to vegetate, to fight against his creditors, to accept the derisive alms of some sinecure, poisoned himself, like a shop girl who is forsaken by her lover.</p>\\\\n<p>Madame de Maurillac did not mourn for him, and as this lamentable disaster had made her interesting, and as she was assisted and supported by unexpected acts of kindness, and had a good adviser in one of those old Parisian lawyers who would get anybody out of the most inextricable difficulties, she managed to save something from the wreck, and to keep a small income. Then reassured and emboldened, and resting her ultimate illusions and her chimerical hopes on her daughter\\\\u2019s radiant beauty, and preparing for that last game in which they would risk everything, and perhaps also hoping that she might herself marry again, the ancient flirt arranged a double existence.</p>\\\\n<p>For months and months she disappeared from the world, and as a pretext for her isolation and for hiding herself in the country, she alleged her daughter\\\\u2019s delicate health, and also the important interests she had to look after in the South of France.</p>\\\\n<p>Her frivolous friends looked upon that as a great act of heroism, as something almost super-human, and so courageous, that they tried to distract her by their incessant letters, religiously kept her up in all the scandal, and love adventures, in the falls, as well as in the apotheosis of the capital.</p>\\\\n<p>The difficult struggle which Madame de Maurillac had to keep up in order to maintain her rank, was really as fine as any of those campaigns in the twilight of glory, as those slow retreats where men only give way inch by inch and fight until the last cartridge is expended, until at last fresh troops arrive, reinforcement which bar the way to the enemy, and save the threatened flag.</p>\\\\n<p>Broken in by the same discipline, and haunted by the same dream, mother and daughter lived on almost nothing in the dull, dilapidated house which the peasants called the <em>ch\\\\u00e2teau</em>, and economized like poor people who only have a few hundred francs a year to live on. But Fabienne de Maurillac developed well in spite of everything, and grew up into a woman like some rare flower which is preserved from all contact with the outer air and is reared in a hot-house.</p>\\\\n<p>In order that she might not lose her Parisian accent by speaking too much with the servants, who had remained peasants under their livery, Madame de Maurillac, who had not been able to bring a lady\\\\u2019s maid with her, on account of the extra cost which her traveling expenses and wages would have entailed, and who, moreover, was afraid that some indiscretion might betray her maneuvers and cover her with ridicule, made up her mind to wait on her daughter herself. And Fabienne talked with nobody but her, saw nobody but her, and was like a little novice in a convent. Nobody was allowed to speak to her, or to interfere with her walks in the large garden, or on the white terraces that were reflected in the blue water.</p>\\\\n<p>As soon as the season for the country and the seaside came, however, they packed up their trunks, and locked the doors of their house of exile. As they were not known, and taking those terrible trains which stop at every station, and by which travelers arrive at their destination in the middle of the night, with the certainty that nobody will be waiting for you, and see you get out of the carriage, they traveled third class, so that they might have a few bank notes the more, with which to make a show.</p>\\\\n<p>A fortnight in Paris in the family house at Auteuil, a fortnight in which to try on dresses and bonnets and to show themselves, and then Trouville, Aix or Biarritz, the whole show complete, with parties succeeding parties, money was spent as if they did not know its value, balls at the Casinos, constant flirtations, compromising intimacies, and those kind of admirers who immediately surround two pretty women, one in the radiant beauty of her eighteen years, and the other in the brightness of that maturity, which beautiful September days bring with them.</p>\\\\n<p>Unfortunately, however, they had to do the same thing over again every year, and as if bad luck were continuing to follow them implacably, Madame de Maurillac and her daughter did not succeed in their endeavors, and did not manage during her usual absence from home, to pick up some nice fellow who fell in love immediately, who took them seriously, and asked for Fabienne\\\\u2019s hand, consequently, they were very unhappy. Their energies flagged, and their courage left them like water that escapes, drop by drop, through a crack in a jug. They grew low-spirited and no longer dared to be open towards each other and to exchange confidences and projects.</p>\\\\n<p>Fabienne, with her pale cheeks, her large eyes with blue circles round them and her tight lips, looked like some captive princess who is tormented by constant ennui, and troubled by evil suggestions; who dreams of flight, and of escape from that prison where fate holds her captive.</p>\\\\n<p>One night, when the sky was covered with heavy thunderclouds and the heat was most oppressive, Madame de Maurillac called her daughter whose room was next to hers. After calling her loudly for some time in vain, she sprang out of bed in terror and almost broke open the door with her trembling hands. The room was empty, and the pillows untouched.</p>\\\\n<p>Then, nearly mad and foreseeing some irreparable misfortune, the poor woman ran all over the large house, and then rushed out into the garden, where the air was heavy with the scent of flowers. She had the appearance of some wild animal that is being pursued by a pack of hounds, tried to penetrate the darkness with her anxious looks, and gasped as if some one were holding her by the throat; but suddenly she staggered, uttered a painful cry and fell down in a fit.</p>\\\\n<p>There before her, in the shadow of the myrtle trees, Fabienne was sitting on the knees of a man \\\\u2014 of the gardener \\\\u2014 with both her arms round his neck and kissing him ardently, and as if to defy her, and to show her how vain all her precautions and her vigilance had been, the girl was telling her lover in the country dialect, and in a cooing and delightful voice, how she adored him and that she belonged to him. .\\\\u00a0.\\\\u00a0.</p>\\\\n<p>Madame de Maurillac is in a lunatic asylum, and Fabienne has married the gardener.</p>\\\\n<p>What could she have done better?</p>\\\\n</div>\\\\n<div class=\\\\\"colophon\\\\\">\\\\n<p>This web edition published by:</p>\\\\n<p>eBooks@Adelaide<br/>\\\\nThe University of Adelaide Library<br/>\\\\nUniversity of Adelaide<br/>\\\\nSouth Australia 5005</p>\\\\n</div>\\\\n</body>\\\\n</html>\\\\n\"]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_page_json.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B Getting Help\n",
    "\n",
    "At any moment, you can get help on a Python object using the `help()` function. For example, if we want to know more aboud the RDD's `take()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method take in module pyspark.rdd:\n",
      "\n",
      "take(num) method of pyspark.rdd.RDD instance\n",
      "    Take the first num elements of the RDD.\n",
      "    \n",
      "    It works by first scanning one partition, and use the results from\n",
      "    that partition to estimate the number of additional partitions needed\n",
      "    to satisfy the limit.\n",
      "    \n",
      "    Translated from the Scala implementation in RDD#take().\n",
      "    \n",
      "    .. note:: this method should only be used if the resulting array is expected\n",
      "        to be small, as all the data is loaded into the driver's memory.\n",
      "    \n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      "    [2, 3]\n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      "    [2, 3, 4, 5, 6]\n",
      "    >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      "    [91, 92, 93]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(adelaide_meta_json.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C Action on a Dataset\n",
    "\n",
    "The `take()` method is one among multiple available *actions* we can apply on an RDD. An exhaustive [list of actions is available in Spark documentation](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions).\n",
    "\n",
    "In case where we do not want to leave the notebook tab, we can call `help()` directly on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(builtins.object)\n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
      " |      C{b} is in C{other}.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with L{SparkContext.setCheckpointDir()} and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in C{self} or C{other}, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in C{self} as\n",
      " |      well as C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting data is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - C{createCombiner}, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n",
      " |            the lists)\n",
      " |      \n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      .. note:: V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <http://dx.doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      :param relativeSD: Relative accuracy. Smaller values create\n",
      " |                         counters that require more space.\n",
      " |                         It must be greater than 0.000017.\n",
      " |      \n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in C{other}, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in C{self}, or the pair\n",
      " |      (k, (None, w)) if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      .. note:: If you are grouping in order to perform an aggregation (such as a\n",
      " |          sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |          provide much better performance.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) inseration to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      The return value is a tuple of buckets and histogram.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      .. note:: This method performs a shuffle internally.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      .. note:: an RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self)\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      C{self} and C{other}.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying C{f}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self)\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      L{spark.dynamicAllocation.cachedExecutorIdleTimeout} to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through L{SparkContext.setCheckpointDir()} is not used.\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Deprecated: use mapPartitionsWithIndex instead.\n",
      " |      \n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (C{MEMORY_ONLY}).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |      \n",
      " |      :param checkCode: whether or not to check the return value of the shell command.\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      " |      :param seed: random seed\n",
      " |      :return: split RDDs in a list\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>)\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with C{numPartitions} partitions, or\n",
      " |      the default parallelism level if C{numPartitions} is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |      Can increase or decrease the level of parallelism in this RDD.\n",
      " |      Internally, this uses a shuffle to redistribute data.\n",
      " |      If you are decreasing the number of partitions in this RDD, consider\n",
      " |      using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |      >>> len(rdd.repartition(2).glom().collect())\n",
      " |      2\n",
      " |      >>> len(rdd.repartition(10).glom().collect())\n",
      " |      10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f6c2c3e4510>, ascending=True, keyfunc=<function RDD.<lambda> at 0x7f6c24194f28>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, w) in C{other}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
      " |      :param fraction: expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      :param seed: seed for the random number generator\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: (None by default)\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is L{pyspark.serializers.PickleSerializer}, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      :param path: path to sequence file\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      @param path: path to text file\n",
      " |      @param compressionCodecClass: (None by default) string i.e.\n",
      " |          \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      \n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda> at 0x7f6c241980d0>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a L{StatCounter} object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in C{self} that is not contained in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in C{self} that has no pair with matching\n",
      " |      key in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      .. note:: this method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: It returns the list sorted in descending order.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      L{zipWithIndex}\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  context\n",
      " |      The L{SparkContext} that this RDD was created on.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(adelaide_meta_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the available actions, there is method named `count()`.\n",
    "\n",
    "#### Exercise 2: How to Count?\n",
    "\n",
    "Call the help function on the count method of the `adelaide_meta_json` to get to know more about the `count()` action. Then, apply this action on both RDDs and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_count = adelaide_meta_json.count()\n",
    "page_count = adelaide_page_json.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4434\n"
     ]
    }
   ],
   "source": [
    "print(meta_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4434\n"
     ]
    }
   ],
   "source": [
    "print(page_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action applied on an RDD leads to the creation of one or many tasks and the production of a result. Every task executed in the same app can be visualized in the Spark's dashboard. In this interface, we can track the progress of a task, and check different performance measures on the task, for example its duration and cache statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.D Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the first 4 elements of our datasets that we retrieved earlier,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"url\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"1844\", \"datePublished\": \"2015-10-26\", \"@type\": \"Book\", \"author\": \"Emerson, Ralph Waldo, 1803-1882\", \"name\": \"New England Reformers\", \"description\": \"New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson\", \"image\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}',\n",
       " '{\"url\": \"https://ebooks.adelaide.edu.au/m/maupassant/guy/new-sensation/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"\", \"datePublished\": \"2016-01-26\", \"@type\": \"Book\", \"author\": \"Maupassant, Guy de, 1850-1893\", \"name\": \"The New Sensation\", \"description\": \"The New Sensation : (Parisine) [] / Guy de Maupassant\", \"image\": \"https://ebooks.adelaide.edu.au/m/maupassant/guy/new-sensation/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}',\n",
       " '{\"author\": \"Lawrence, D. H. (David Herbert), 1885-1930\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"inLanguage\": \"en\", \"dateCreated\": \"1934\", \"name\": \"New Eve and Old Adam\", \"@type\": \"Book\", \"url\": \"https://ebooks.adelaide.edu.au/l/lawrence/dh/new-eve-and-old-adam/\", \"image\": \"https://ebooks.adelaide.edu.au/l/lawrence/dh/new-eve-and-old-adam/cover.jpg\", \"description\": \"New Eve and Old Adam / D. H. Lawrence\", \"publisher\": \"The University of Adelaide Library\"}',\n",
       " '{\"url\": \"https://ebooks.adelaide.edu.au/h/hume/david/h92n/\", \"@context\": \"http://schema.org\", \"contributors\": [\"Robertson, John Mackinnon, 1856-1933\", \"Hume, David, 1711-1776\"], \"dateCreated\": \"1757\", \"datePublished\": \"2009-12-09\", \"dateModified\": \"2014-02-26\", \"@type\": \"Book\", \"author\": \"Hume, David, 1711-1776\", \"name\": \"The Natural History of Religion\", \"keywords\": \"Philosophy\", \"description\": \"The Natural History of Religion / David Hume; with an introduction by John M. Robertson\", \"image\": \"https://ebooks.adelaide.edu.au/h/hume/david/h92n/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_first4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we realize that the RDD is composed of the lines from the input text files. However, it is not possible to access to individual field in each dictionnary. **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"url\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/\", \"@context\": \"http://schema.org\", \"keywords\": \"Literature\", \"dateCreated\": \"1844\", \"datePublished\": \"2015-10-26\", \"@type\": \"Book\", \"author\": \"Emerson, Ralph Waldo, 1803-1882\", \"name\": \"New England Reformers\", \"description\": \"New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson\", \"image\": \"https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg\", \"publisher\": \"The University of Adelaide Library\", \"inLanguage\": \"en\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_first = adelaide_meta_json.first()\n",
    "meta_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b6d63ff712b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeta_first\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "meta_first['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://schema.org\", '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_first[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action `first()` as its name states, return the first entry of the dataset. We see that **each entry is a single string**. We will need to transform each entry of the RDD in order to convert the strings, encoded in JSON, into a Python dictionary. To do this, we will use the Python standard library function **`json.loads`** to convert each JSON encoded string into its Python equivalent.\n",
    "\n",
    "First, lets test `json.loads` on the previous first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/',\n",
       " '@context': 'http://schema.org',\n",
       " 'keywords': 'Literature',\n",
       " 'dateCreated': '1844',\n",
       " 'datePublished': '2015-10-26',\n",
       " '@type': 'Book',\n",
       " 'author': 'Emerson, Ralph Waldo, 1803-1882',\n",
       " 'name': 'New England Reformers',\n",
       " 'description': 'New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson',\n",
       " 'image': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg',\n",
       " 'publisher': 'The University of Adelaide Library',\n",
       " 'inLanguage': 'en'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(meta_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_first_dict = json.loads(meta_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/\n"
     ]
    }
   ],
   "source": [
    "print(meta_first_dict['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to apply this transformation to every entry in the RDD. The RDD's method `map(func)` returns a new distributed dataset formed by applying on each element of the source a function *func*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta = adelaide_meta_json.map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of this transformation is *lazy*. Spark does not compute anything as long as a result is not requested by an action. To convince yourself, execute the preceding cell, then visit the Spark dashboard. You should see that no job have been added to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince ourselves that the transformation will be successfully applied, we can retrieve the first element of the transformed RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/', '@context': 'http://schema.org', 'keywords': 'Literature', 'dateCreated': '1844', 'datePublished': '2015-10-26', '@type': 'Book', 'author': 'Emerson, Ralph Waldo, 1803-1882', 'name': 'New England Reformers', 'description': 'New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson', 'image': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg', 'publisher': 'The University of Adelaide Library', 'inLanguage': 'en'}\n",
      "https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/\n"
     ]
    }
   ],
   "source": [
    "print(adelaide_meta.first())\n",
    "print(adelaide_meta.first()['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: How to Transform?\n",
    "\n",
    "Apply the JSON transformation on the page RDD that we have created in exercise 1 and print the URL of the fifth element of that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page = adelaide_page_json.map(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ebooks.adelaide.edu.au/m/maupassant/guy/accent/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_page.first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.E Filtering a Dataset\n",
    "\n",
    "#### 3.E.1 Filtering Bad Entries\n",
    "\n",
    "Since we now have RDDs that are easier to manipulate, we can start the analysis. \n",
    "\n",
    "Our dataset was built by scraping the webpages of Adelaide University. However, during the process, some of the webpages could not be fetched by our spider pogram. Therefore, in our dataset, we end up with two kinds of entry.\n",
    "\n",
    "Good entry example:\n",
    "```\n",
    "{\"@context\": \"http://schema.org\", \"dateModified\": \"2014-02-26\", \"image\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/cover.jpg\", \"author\": \"Bowen, Marjorie, 1885-1952\", \"@type\": \"Book\", \"source\": \"https://gutenberg.net.au/ebooks09/0900581.txt\", \"inLanguage\": \"en\", \"publisher\": \"The University of Adelaide Library\", \"name\": \"The Avenging of Ann Leete\", \"keywords\": \"Literature\", \"url\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/\", \"description\": \"The Avenging of Ann Leete / Marjorie Bowen\"}\n",
    "```\n",
    "\n",
    "Bad entry example:\n",
    "```\n",
    "{\"description\": \"ERROR_COMP_NOT_FOUND\"}\n",
    "```\n",
    "\n",
    "For the next operation, we wish to only keep entries for which we at least know the name of the author and the title of the book. To do so, we first define a function that returns `True` if the fields `author` and `name` are in the dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_author_title_defined(record):\n",
    "    return \"author\" and \"name\" in record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to answer the following quiz before executing the cell:  \n",
    "* What sort of argument takes the `filter()` method?\n",
    "* Is filter an action or a transformation?\n",
    "* What does `filter()` return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_filt = adelaide_meta.filter(is_author_title_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4366"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_filt.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_for_more = adelaide_meta.filter(lambda x: not is_author_title_defined(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'inLanguage': 'eng',\n",
       "  '@context': 'http://schema.org',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  '@type': 'Book',\n",
       "  'url': 'https://ebooks.adelaide.edu.au/f/fourier/joseph/heat/',\n",
       "  'author': 'Fourier, Jean Baptiste Joseph, baron, 1768-1830.',\n",
       "  'description': 'The Analytical Theory of Heat / Joseph Fourier; translated, with notes, by Alexander Freeman'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'author': 'Ruysbroeck, Willem van, ca. 1210-ca. 1270.',\n",
       "  '@context': 'http://schema.org',\n",
       "  'inLanguage': 'eng',\n",
       "  'url': 'https://ebooks.adelaide.edu.au/h/hakluyt/voyages/v02/rubruquis/',\n",
       "  '@type': 'Book',\n",
       "  'description': 'The iournal of frier William de Rubruquis a French man of the order of the minorite friers, vnto the East parts of the worlde. An. Dom. 1253. / William de Rubruquis',\n",
       "  'publisher': 'The University of Adelaide Library'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'inLanguage': 'eng',\n",
       "  '@context': 'http://schema.org',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  '@type': 'Book',\n",
       "  'url': 'https://ebooks.adelaide.edu.au/g/gastronomy/country_housewife/',\n",
       "  'author': 'Bradley, Richard, 1688-1732.',\n",
       "  'description': 'The Country Housewife and Lady’s Director in the Management of a House, and the Delights and Profits of a Farm / Richard Bradley'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'author': None,\n",
       "  '@context': 'http://schema.org',\n",
       "  'inLanguage': 'eng',\n",
       "  'url': 'https://ebooks.adelaide.edu.au/r/religion/sacred_books_and_early_literature_of_the_east/',\n",
       "  '@type': 'Book',\n",
       "  'description': 'The sacred books and early literature of the East; with historical surveys of the chief writings of each Nation',\n",
       "  'publisher': 'The University of Adelaide Library'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'author': 'Symonds, John Addington, 1840-1893.',\n",
       "  '@context': 'http://schema.org',\n",
       "  'url': 'https://ebooks.adelaide.edu.au/l/literature/english-men-of-letters/shelley/',\n",
       "  'inLanguage': 'en',\n",
       "  'dateCreated': '1878',\n",
       "  '@type': 'Book',\n",
       "  'description': 'Shelley / John Addington Symonds',\n",
       "  'publisher': 'The University of Adelaide Library'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'},\n",
       " {'description': 'ERROR_COMP_NOT_FOUND'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "look_for_more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: How to Filter?\n",
    "\n",
    "For the next exercise, you will design your own bad entry filter. The page RDD's entries are not dictonaries but lists. Here is an example of a bad entry:\n",
    "```\n",
    "[\"https://ebooks.adelaide.edu.au/d/dante/\", \"None\"]\n",
    "```\n",
    "\n",
    "Write a function that will return `True` or `False` wether the entry is good or bad then create a new RDD named `adelaide_meta_filt` by applying your filter function to every entry of adelaide_page.\n",
    "\n",
    "To assess your filter design, count the number of elements in the resulting RDD. How many entries have you filtered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_entry_example = [\"https://ebooks.adelaide.edu.au/d/dante/\", \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_entry_example[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_entry_example[1] == 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_entry_example[1] != 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bad_entry(entry):\n",
    "    return entry[1] != 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_bad_entry(bad_entry_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_bad_entry(adelaide_page.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_filt = adelaide_page.filter(filter_bad_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4388"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_page_filt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.E.2 Filtering Duplicate Entries\n",
    "\n",
    "The meta-information of each book have been recovered by scraping the website of [Adelaide University's eBook Libary](https://ebooks.adelaide.edu.au/). Since two pages could point to the same book, there is a possibility that a book is present more than once in our dataset.\n",
    "\n",
    "#### Exercise 5: How to Extract?\n",
    "\n",
    "**To confirm that some books are present more than once in our dataset,  transform the dataset `adelaide_meta_filt` in a second dataset that only includes URLs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_urls = adelaide_meta_filt.map(lambda dictionary: dictionary['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs have a special method named `distinct()` that returns a new RDD containing strictly unique values. Next, we are going to call this method on our RDD of URLs and count the number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_urls.count() - meta_urls.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 305 duplicated entries in our dataset. To remove the duplicated entries, we will need to first associate an identifier that should be unique to each entry. We will name this identifier \"key\". A unique identifier for a webpage is its URL. Since every book is associated to a URL, we will use the URLs as the keys to our entries.\n",
    "\n",
    "The RDD method `keyBy()` allows to associate each entry in our dataset with a key. The key will be defined from an item of the record, in this case the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_url_key = adelaide_meta_filt.keyBy(lambda rec: rec['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry now has its own key, to convince ourselves, we can fetch the first element of that last RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/',\n",
       " '@context': 'http://schema.org',\n",
       " 'keywords': 'Literature',\n",
       " 'dateCreated': '1844',\n",
       " 'datePublished': '2015-10-26',\n",
       " '@type': 'Book',\n",
       " 'author': 'Emerson, Ralph Waldo, 1803-1882',\n",
       " 'name': 'New England Reformers',\n",
       " 'description': 'New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson',\n",
       " 'image': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg',\n",
       " 'publisher': 'The University of Adelaide Library',\n",
       " 'inLanguage': 'en'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_filt.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = adelaide_meta_filt.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/',\n",
       " {'url': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/',\n",
       "  '@context': 'http://schema.org',\n",
       "  'keywords': 'Literature',\n",
       "  'dateCreated': '1844',\n",
       "  'datePublished': '2015-10-26',\n",
       "  '@type': 'Book',\n",
       "  'author': 'Emerson, Ralph Waldo, 1803-1882',\n",
       "  'name': 'New England Reformers',\n",
       "  'description': 'New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson',\n",
       "  'image': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  'inLanguage': 'en'})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rec['url'], rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/',\n",
       " {'url': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/',\n",
       "  '@context': 'http://schema.org',\n",
       "  'keywords': 'Literature',\n",
       "  'dateCreated': '1844',\n",
       "  'datePublished': '2015-10-26',\n",
       "  '@type': 'Book',\n",
       "  'author': 'Emerson, Ralph Waldo, 1803-1882',\n",
       "  'name': 'New England Reformers',\n",
       "  'description': 'New England Reformers : A Lecture read before the Society in Amory Hall, on Sunday, 3 March, 1844 / Ralph Waldo Emerson',\n",
       "  'image': 'https://ebooks.adelaide.edu.au/e/emerson/ralph_waldo/new-england-reformers/cover.jpg',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  'inLanguage': 'en'})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_url_key.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to keep only one value for each entry that shares the same key. To do this, we will apply a reduction operation, such that if we are given two records `a` and `b` with the same key, we only return the first record `a`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_unique = adelaide_meta_url_key.reduceByKey(lambda a, b: a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This series of operations are called a reduction operation on a key-value pair RDD. We will provide more details in two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: How to Reduce?\n",
    "\n",
    "The page dataset may also include duplicated pages. Identify what should be the key of that dataset then try to create a new dataset with only unique books. What is the structure of a record after applying the `keyBy()` method? Do we need to transform this dataset or could it be reduced directly?\n",
    "\n",
    "Count the number of elements in the resulting RDD to confirm the validity your transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_unique = adelaide_page_filt.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we no longer need the keys, we can retrieve the values by calling the `values()` method on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_filt.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/',\n",
       " {'url': 'https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/',\n",
       "  '@context': 'http://schema.org',\n",
       "  'dateCreated': '1880',\n",
       "  'datePublished': '2003-03-16',\n",
       "  'dateModified': '2014-03-04',\n",
       "  '@type': 'Book',\n",
       "  'author': 'Zola, Émile, 1840-1902',\n",
       "  'name': 'Nana',\n",
       "  'description': 'Nana / Emile Zola',\n",
       "  'image': 'https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/cover.jpg',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  'inLanguage': 'fr'})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_unique.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_uniq = adelaide_meta_unique.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.F Caching a Dataset\n",
    "\n",
    "When we expect to operate frequently on the same dataset, it can be useful to tell Spark to keep it in memory.\n",
    "\n",
    "To do so, we use the `cache()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[40] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_uniq.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDDs stored in memory are displayed in the **Storage** section of Spark web interface. Note that datasets are not loaded in memory until an action is made on them. \n",
    "\n",
    "Actions on cached datasets are much faster than on non-cached datasets. But in order to be cached, an action must first be applied on the dataset. Based on that, try to explain the execution time for the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.47 ms, sys: 2.84 ms, total: 11.3 ms\n",
      "Wall time: 472 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time adelaide_meta_uniq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.14 ms, sys: 5.05 ms, total: 9.19 ms\n",
      "Wall time: 146 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time adelaide_meta_uniq.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7: How to Cache?\n",
    "\n",
    "Cache the RDD from exercise 6 and evaluate how long it takes to retrieve the first 5 elements before and after caching. \n",
    "\n",
    "What happens when there is not enough memory to cache an RDD? \n",
    "\n",
    "Can you figure how to *uncache* an RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_unique.<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pages_5 = adelaide_page_unique.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pages_5 = adelaide_page_unique.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_unique.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing\n",
    "\n",
    "<img src=\"http://www.lightspeedgmi.com/wp-content/uploads/2015/03/dataprocessing-circle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grasp the extent of our dataset, we can count to number of entries it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_uniq.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/',\n",
       " '@context': 'http://schema.org',\n",
       " 'dateCreated': '1880',\n",
       " 'datePublished': '2003-03-16',\n",
       " 'dateModified': '2014-03-04',\n",
       " '@type': 'Book',\n",
       " 'author': 'Zola, Émile, 1840-1902',\n",
       " 'name': 'Nana',\n",
       " 'description': 'Nana / Emile Zola',\n",
       " 'image': 'https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/cover.jpg',\n",
       " 'publisher': 'The University of Adelaide Library',\n",
       " 'inLanguage': 'fr'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_uniq.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry of our dataset is a dictionary. Each dictionary can contain a variable number of keys and every dictionary in our dataset do not necessarily share the same keys.\n",
    "\n",
    "We can extract the keys from each dictionary and count how many times they are present. To access, the key of a dictionary, we can use the method `keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', '@context', 'dateCreated', 'datePublished', 'dateModified', '@type', 'author', 'name', 'description', 'image', 'publisher', 'inLanguage'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_uniq.first().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['url',\n",
       "  '@context',\n",
       "  'dateCreated',\n",
       "  'datePublished',\n",
       "  'dateModified',\n",
       "  '@type',\n",
       "  'author',\n",
       "  'name',\n",
       "  'description',\n",
       "  'image',\n",
       "  'publisher',\n",
       "  'inLanguage'],\n",
       " ['author',\n",
       "  '@context',\n",
       "  'keywords',\n",
       "  'inLanguage',\n",
       "  'url',\n",
       "  'dateModified',\n",
       "  'name',\n",
       "  '@type',\n",
       "  'image',\n",
       "  'description',\n",
       "  'publisher']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_uniq.map(lambda rec: list(rec.keys())).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to apply this method to every dictionary in our dataset, so that would be a map. However, if we simply apply a map, we will get an RDD of key-lists. What we truly want is to merge the list to get an RDD of keys. \n",
    "\n",
    "Spark has a function to merge the iterable returned by a function, the `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_keys = adelaide_meta_uniq.flatMap(lambda rec: list(rec.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url',\n",
       " '@context',\n",
       " 'dateCreated',\n",
       " 'datePublished',\n",
       " 'dateModified',\n",
       " '@type',\n",
       " 'author',\n",
       " 'name',\n",
       " 'description',\n",
       " 'image',\n",
       " 'publisher',\n",
       " 'inLanguage',\n",
       " 'author',\n",
       " '@context',\n",
       " 'keywords',\n",
       " 'inLanguage',\n",
       " 'url',\n",
       " 'dateModified',\n",
       " 'name',\n",
       " '@type']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_keys.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first 5 elements of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_keys.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in finding the frequency of each key. This will give us an idea of the completeness of our dataset. To compute the key frequency, we will need to apply a classic map-reduce pattern.\n",
    "\n",
    "First, we need to pair each key with the basic frequency value 1. This is the map operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_key_value = adelaide_keys.map(lambda key: (key, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the result of the transformation by looking at the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('url', 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_key_value.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will sum the values that shares the same key. This is the reduce operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "# adelaide_agg = adelaide_key_value.reduceByKey(add)\n",
    "adelaide_agg = adelaide_key_value.reduceByKey(lambda a,b : a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can collect our transformed RDD. Key-Value pair RDDs have a special method `collectAsMap` that returns the result as a dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_agg.collectAsMap()['@context']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that only a few keys are available in most dictionaries of our dataset. We should therefore restrict our analysis to these fields or create new fields from the frequent one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A Valorizing data by transforming the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_name_birth_death(record):\n",
    "    author = record.get('author', None)\n",
    "    if author:\n",
    "        author = author.strip()\n",
    "        # Remove trailing dot\n",
    "        if '.' == author[-1]:\n",
    "            author = author[:-1]\n",
    "        try:\n",
    "            lastname, firstname, birth_death = author.split(',')\n",
    "        except ValueError:\n",
    "            return record\n",
    "        try:\n",
    "            birth, death = re.findall('\\d+', birth_death)\n",
    "        except ValueError:\n",
    "            return record\n",
    "        record['author_lastname'] = lastname.strip()\n",
    "        record['author_firstname'] = firstname.strip()\n",
    "        record['author_birth'] = int(birth)\n",
    "        record['author_death'] = int(death)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_val = adelaide_meta_unique.mapValues(process_name_birth_death).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/',\n",
       "  '@context': 'http://schema.org',\n",
       "  'dateCreated': '1880',\n",
       "  'datePublished': '2003-03-16',\n",
       "  'dateModified': '2014-03-04',\n",
       "  '@type': 'Book',\n",
       "  'author': 'Zola, Émile, 1840-1902',\n",
       "  'name': 'Nana',\n",
       "  'description': 'Nana / Emile Zola',\n",
       "  'image': 'https://ebooks.adelaide.edu.au/z/zola/emile/z8nf/cover.jpg',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  'inLanguage': 'fr',\n",
       "  'author_lastname': 'Zola',\n",
       "  'author_firstname': 'Émile',\n",
       "  'author_birth': 1840,\n",
       "  'author_death': 1902},\n",
       " {'author': 'Buchan, John, 1875-1940',\n",
       "  '@context': 'http://schema.org',\n",
       "  'keywords': 'Literature',\n",
       "  'inLanguage': 'en',\n",
       "  'url': 'https://ebooks.adelaide.edu.au/b/buchan/john/no_man_s_land/',\n",
       "  'dateModified': '2014-02-26',\n",
       "  'name': 'No-Man’s-Land',\n",
       "  '@type': 'Book',\n",
       "  'image': 'https://ebooks.adelaide.edu.au/b/buchan/john/no_man_s_land/cover.jpg',\n",
       "  'description': 'No-Man’s-Land / John Buchan',\n",
       "  'publisher': 'The University of Adelaide Library',\n",
       "  'author_lastname': 'Buchan',\n",
       "  'author_firstname': 'John',\n",
       "  'author_birth': 1875,\n",
       "  'author_death': 1940}]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adelaide_meta_val.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dateCreated(record):\n",
    "    if 'dateCreated' in record:\n",
    "        dates = re.findall('\\d+', record['dateCreated'])\n",
    "        if len(dates) > 0:\n",
    "            date = int(dates[0])\n",
    "            # Check if the date is before common era\n",
    "            if re.match(r'BC|bc|BCE|bce', record['dateCreated']):\n",
    "                date *= -1\n",
    "            record['dateCreated'] = date\n",
    "        else:\n",
    "            del record['dateCreated']\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_val = adelaide_meta_val.map(convert_dateCreated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B First analysis: authors' life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_age(rec):\n",
    "    \"\"\"Compute the age of an author when it died\n",
    "    based on its year of birth and death.\n",
    "    \"\"\"\n",
    "    if 'author_birth' and 'author_death' in rec:\n",
    "        birth, death =  rec['author_birth'], rec['author_death']\n",
    "        if birth < death:\n",
    "            return death - birth\n",
    "        else:\n",
    "            # If year of birth is greater than year of death the\n",
    "            # author was born in BCE. Do you think this is correct\n",
    "            # in every cases?\n",
    "            return birth - death\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "age_frequency = adelaide_meta_val.map(compute_age)\\\n",
    "                                 .countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization can be done with multiple tools, here we use plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          62,
          65,
          42,
          43,
          74,
          56,
          84,
          52,
          null,
          47,
          33,
          63,
          90,
          82,
          46,
          88,
          44,
          68,
          61,
          39,
          73,
          60,
          78,
          38,
          69,
          51,
          54,
          80,
          58,
          67,
          66,
          71,
          45,
          64,
          75,
          55,
          81,
          41,
          77,
          40,
          34,
          89,
          57,
          59,
          49,
          72,
          37,
          30,
          70,
          83,
          93,
          85,
          36,
          48,
          26,
          79,
          53,
          94,
          76,
          29,
          32,
          35,
          86,
          91,
          50,
          87,
          92,
          31,
          101,
          27
         ],
         "y": [
          79,
          170,
          9,
          318,
          70,
          54,
          26,
          97,
          478,
          103,
          6,
          16,
          10,
          110,
          96,
          37,
          96,
          96,
          59,
          12,
          217,
          48,
          77,
          50,
          48,
          86,
          20,
          146,
          97,
          130,
          108,
          86,
          58,
          39,
          83,
          83,
          74,
          13,
          88,
          61,
          10,
          22,
          25,
          91,
          11,
          48,
          12,
          9,
          58,
          35,
          8,
          15,
          7,
          21,
          13,
          37,
          3,
          11,
          22,
          11,
          2,
          7,
          7,
          1,
          11,
          5,
          2,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "title": "Adelaide authors life expectancy",
        "xaxis": {
         "title": "life expectancy (years)"
        },
        "yaxis": {
         "title": "number of authors"
        }
       }
      },
      "text/html": [
       "<div id=\"4aa56914-98b5-4341-8d1b-74e54a9a0266\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4aa56914-98b5-4341-8d1b-74e54a9a0266\", [{\"type\": \"bar\", \"x\": [62, 65, 42, 43, 74, 56, 84, 52, null, 47, 33, 63, 90, 82, 46, 88, 44, 68, 61, 39, 73, 60, 78, 38, 69, 51, 54, 80, 58, 67, 66, 71, 45, 64, 75, 55, 81, 41, 77, 40, 34, 89, 57, 59, 49, 72, 37, 30, 70, 83, 93, 85, 36, 48, 26, 79, 53, 94, 76, 29, 32, 35, 86, 91, 50, 87, 92, 31, 101, 27], \"y\": [79, 170, 9, 318, 70, 54, 26, 97, 478, 103, 6, 16, 10, 110, 96, 37, 96, 96, 59, 12, 217, 48, 77, 50, 48, 86, 20, 146, 97, 130, 108, 86, 58, 39, 83, 83, 74, 13, 88, 61, 10, 22, 25, 91, 11, 48, 12, 9, 58, 35, 8, 15, 7, 21, 13, 37, 3, 11, 22, 11, 2, 7, 7, 1, 11, 5, 2, 1, 1, 1]}], {\"title\": \"Adelaide authors life expectancy\", \"xaxis\": {\"title\": \"life expectancy (years)\"}, \"yaxis\": {\"title\": \"number of authors\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"4aa56914-98b5-4341-8d1b-74e54a9a0266\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4aa56914-98b5-4341-8d1b-74e54a9a0266\", [{\"type\": \"bar\", \"x\": [62, 65, 42, 43, 74, 56, 84, 52, null, 47, 33, 63, 90, 82, 46, 88, 44, 68, 61, 39, 73, 60, 78, 38, 69, 51, 54, 80, 58, 67, 66, 71, 45, 64, 75, 55, 81, 41, 77, 40, 34, 89, 57, 59, 49, 72, 37, 30, 70, 83, 93, 85, 36, 48, 26, 79, 53, 94, 76, 29, 32, 35, 86, 91, 50, 87, 92, 31, 101, 27], \"y\": [79, 170, 9, 318, 70, 54, 26, 97, 478, 103, 6, 16, 10, 110, 96, 37, 96, 96, 59, 12, 217, 48, 77, 50, 48, 86, 20, 146, 97, 130, 108, 86, 58, 39, 83, 83, 74, 13, 88, 61, 10, 22, 25, 91, 11, 48, 12, 9, 58, 35, 8, 15, 7, 21, 13, 37, 3, 11, 22, 11, 2, 7, 7, 1, 11, 5, 2, 1, 1, 1]}], {\"title\": \"Adelaide authors life expectancy\", \"xaxis\": {\"title\": \"life expectancy (years)\"}, \"yaxis\": {\"title\": \"number of authors\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    go.Bar(\n",
    "        x=list(age_frequency.keys()),\n",
    "        y=list(age_frequency.values()),\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Adelaide authors life expectancy\",\n",
    "    xaxis=dict(\n",
    "        title='life expectancy (years)',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='number of authors',\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A surprising number of authors died at the age of 43. There is either a pattern with authors, or we have commited a mistake in our analysis.\n",
    "\n",
    "What happens if an author has written more than one book? We need to remember that our dataset is composed of books, not authors. If we want to produce statistics on authors, we need to keep only distinct authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.C Second analysis: authors' life expectancy... done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_name_age(rec):\n",
    "    age = compute_age(rec)\n",
    "    lastname = rec['author_lastname']\n",
    "    firstname = rec['author_firstname']\n",
    "    return firstname, lastname, age\n",
    "\n",
    "authors = adelaide_meta_val.filter(lambda rec: 'author_lastname' in rec)\\\n",
    "                           .map(retrieve_name_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors = authors.distinct()\n",
    "age_frequency2 = unique_authors.map(lambda tup: tup[2]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          77,
          68,
          65,
          79,
          80,
          62,
          47,
          85,
          50,
          75,
          78,
          53,
          54,
          70,
          27,
          60,
          51,
          59,
          43,
          72,
          61,
          69,
          58,
          82,
          94,
          52,
          35,
          57,
          66,
          71,
          84,
          63,
          44,
          45,
          49,
          76,
          73,
          55,
          67,
          56,
          64,
          48,
          83,
          74,
          29,
          42,
          87,
          40,
          30,
          88,
          81,
          31,
          90,
          37,
          92,
          91,
          101,
          41,
          89,
          26,
          86,
          38,
          34,
          46,
          33,
          39,
          32,
          36,
          93
         ],
         "y": [
          11,
          19,
          18,
          10,
          21,
          12,
          11,
          11,
          6,
          20,
          12,
          3,
          9,
          13,
          1,
          5,
          9,
          16,
          5,
          15,
          15,
          17,
          16,
          15,
          1,
          6,
          4,
          17,
          17,
          15,
          4,
          14,
          6,
          6,
          4,
          6,
          13,
          8,
          10,
          10,
          10,
          10,
          13,
          12,
          4,
          3,
          4,
          5,
          3,
          8,
          10,
          1,
          5,
          6,
          2,
          1,
          1,
          4,
          5,
          3,
          3,
          6,
          4,
          13,
          4,
          6,
          1,
          1,
          1
         ]
        }
       ],
       "layout": {
        "title": "Adelaide authors life expectancy",
        "xaxis": {
         "title": "life expectancy (years)"
        },
        "yaxis": {
         "title": "number of authors"
        }
       }
      },
      "text/html": [
       "<div id=\"d37008f6-a177-4b85-8791-a433ac56762d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d37008f6-a177-4b85-8791-a433ac56762d\", [{\"type\": \"bar\", \"x\": [77, 68, 65, 79, 80, 62, 47, 85, 50, 75, 78, 53, 54, 70, 27, 60, 51, 59, 43, 72, 61, 69, 58, 82, 94, 52, 35, 57, 66, 71, 84, 63, 44, 45, 49, 76, 73, 55, 67, 56, 64, 48, 83, 74, 29, 42, 87, 40, 30, 88, 81, 31, 90, 37, 92, 91, 101, 41, 89, 26, 86, 38, 34, 46, 33, 39, 32, 36, 93], \"y\": [11, 19, 18, 10, 21, 12, 11, 11, 6, 20, 12, 3, 9, 13, 1, 5, 9, 16, 5, 15, 15, 17, 16, 15, 1, 6, 4, 17, 17, 15, 4, 14, 6, 6, 4, 6, 13, 8, 10, 10, 10, 10, 13, 12, 4, 3, 4, 5, 3, 8, 10, 1, 5, 6, 2, 1, 1, 4, 5, 3, 3, 6, 4, 13, 4, 6, 1, 1, 1]}], {\"title\": \"Adelaide authors life expectancy\", \"xaxis\": {\"title\": \"life expectancy (years)\"}, \"yaxis\": {\"title\": \"number of authors\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d37008f6-a177-4b85-8791-a433ac56762d\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d37008f6-a177-4b85-8791-a433ac56762d\", [{\"type\": \"bar\", \"x\": [77, 68, 65, 79, 80, 62, 47, 85, 50, 75, 78, 53, 54, 70, 27, 60, 51, 59, 43, 72, 61, 69, 58, 82, 94, 52, 35, 57, 66, 71, 84, 63, 44, 45, 49, 76, 73, 55, 67, 56, 64, 48, 83, 74, 29, 42, 87, 40, 30, 88, 81, 31, 90, 37, 92, 91, 101, 41, 89, 26, 86, 38, 34, 46, 33, 39, 32, 36, 93], \"y\": [11, 19, 18, 10, 21, 12, 11, 11, 6, 20, 12, 3, 9, 13, 1, 5, 9, 16, 5, 15, 15, 17, 16, 15, 1, 6, 4, 17, 17, 15, 4, 14, 6, 6, 4, 6, 13, 8, 10, 10, 10, 10, 13, 12, 4, 3, 4, 5, 3, 8, 10, 1, 5, 6, 2, 1, 1, 4, 5, 3, 3, 6, 4, 13, 4, 6, 1, 1, 1]}], {\"title\": \"Adelaide authors life expectancy\", \"xaxis\": {\"title\": \"life expectancy (years)\"}, \"yaxis\": {\"title\": \"number of authors\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    go.Bar(\n",
    "        x=list(age_frequency2.keys()),\n",
    "        y=list(age_frequency2.values()),\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Adelaide authors life expectancy\",\n",
    "    xaxis=dict(\n",
    "        title='life expectancy (years)',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='number of authors',\n",
    "    ),    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storage\n",
    "\n",
    "<img src=\"\">\n",
    "\n",
    "We could be interested in saving the result of our processing on disk. RDDs have some `saveAs[...]` methods to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_meta_uniq.saveAsTextFile('<FILL IN>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applying new knowledge\n",
    "### 6.A Preprocessing the pages to extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, book1 = adelaide_page.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_page_unique = adelaide_page.groupByKey()\\\n",
    "                                    .mapValues(list)\\\n",
    "                                    .mapValues(itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(page):\n",
    "    if page:\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        it = chain(soup.findAll(['meta', 'script', 'head']),\n",
    "                   soup.findAll('div', {\"id\" : \"controls\"}),\n",
    "                   soup.findAll('div', {\"class\" : \"contents\"}),\n",
    "                   soup.findAll('div', {\"class\" : \"titleverso\"}),\n",
    "                   soup.findAll('div', {\"class\" : \"colophon\"}),\n",
    "                   soup.findAll('span', {\"class\" : \"author\"}))\n",
    "        for div in it:\n",
    "            div.extract()\n",
    "        return soup.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_text = adelaide_page_unique.mapValues(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.B Processing: Analysing the work of an era\n",
    "\n",
    "We have an dataset of 4371 different books written at different times. Lets suppose we want to study the texts of the books written during the 1901–1939 Modernism era.\n",
    "\n",
    "First we need to identify which books in our dataset were written during this era."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_era_books = adelaide_meta_val.filter(lambda rec: 1900 < rec.get('dateCreated', 0) < 1938)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these books are not necessarily in English. We therefore need to apply a second filter on the language (`inLanguage`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_era_en_books = modern_era_books.filter(lambda rec: rec.get('inLanguage', '') == 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now count how many English books from our dataset were written during this era."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_era_en_books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the number of distinct authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_era_books.map(lambda rec: rec['author']).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meta information about each book and the book's text are stored in two separate RDDs. In order to retrieve the texts written during the modernism era, we will need to join the RDD of modern book era metainformation and the RDD of books' text.\n",
    "\n",
    "To do so, we will first need to define the modern era book RDD as an RDD of key-value pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_era_books_kv = modern_era_books.keyBy(lambda rec: rec['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can join the RDD of meta information on modern era books with the RDD of books' text. \n",
    "\n",
    "However, we first need to address a small problem, the URLs of the texts do not exactly match the ones from the meta information. To join two RDDS, the keys need to match perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelaide_text2 = adelaide_text.map(lambda x: (x[0].rsplit('/', 1)[0] + '/', x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding line, we removed the URL suffix which consist of the page name (i.e: `complete.html`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modernism_meta_text = modern_era_books_kv.join(adelaide_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build our corpus of modernism words by creating a single list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word = modernism_meta_text.values().flatMap(lambda x: x[1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def remove_punctuations(word):\n",
    "    return re.sub(r'[{}‘—’”“]'.format(punctuation), \" \", word).strip()\n",
    "\n",
    "stopwords  = set(['all', 'pointing', 'four', 'go', 'oldest', 'seemed', 'whose', 'certainly',\n",
    "'young',  'presents', 'to', 'asking', 'those', 'under', 'far', 'every',\n",
    "'presented', 'did',  'turns', 'large', 'p', 'small', 'parted', 'smaller',\n",
    "'says', 'second', 'further',  'even', 'what', 'anywhere', 'above', 'new',\n",
    "'ever', 'full', 'men', 'here', 'youngest',  'let', 'groups', 'others', 'alone',\n",
    "'along', 'great', 'k', 'put', 'everybody', 'use',  'from', 'working', 'two',\n",
    "'next', 'almost', 'therefore', 'taken', 'until', 'today',  'more', 'knows',\n",
    "'clearly', 'becomes', 'it', 'downing', 'everywhere', 'known', 'cases',  'must',\n",
    "'me', 'states', 'room', 'f', 'this', 'work', 'itself', 'can', 'mr', 'making',\n",
    "'my', 'numbers', 'give', 'high', 'something', 'want', 'needs', 'end', 'turn',\n",
    "'rather', 'how', 'y', 'may', 'after', 'such', 'man', 'a', 'q', 'so', 'keeps',\n",
    "'order', 'furthering',  'over', 'years', 'ended', 'through', 'still', 'its',\n",
    "'before', 'group', 'somewhere',  'interesting', 'better', 'differently',\n",
    "'might', 'then', 'non', 'good', 'somebody',  'greater', 'downs', 'they', 'not',\n",
    "'now', 'gets', 'always', 'l', 'each', 'went', 'side',  'everyone', 'year',\n",
    "'our', 'out', 'opened', 'since', 'got', 'shows', 'turning', 'differ',  'quite',\n",
    "'members', 'ask', 'wanted', 'g', 'could', 'needing', 'keep', 'thing', 'place',\n",
    "'w', 'think', 'first', 'already', 'seeming', 'number', 'one', 'done',\n",
    "'another', 'open',  'given', 'needed', 'ordering', 'least', 'anyone', 'their',\n",
    "'too', 'gives', 'interests',  'mostly', 'behind', 'nobody', 'took', 'part',\n",
    "'herself', 'than', 'kind', 'b', 'showed',  'older', 'likely', 'r', 'were',\n",
    "'toward', 'and', 'sees', 'turned', 'few', 'say', 'have',  'need', 'seem',\n",
    "'saw', 'orders', 'that', 'also', 'take', 'which', 'wanting', 'sure', 'shall',\n",
    "'knew', 'wells', 'most', 'nothing', 'why', 'parting', 'noone', 'later', 'm',\n",
    "'mrs', 'points', 'fact', 'show', 'ending', 'find', 'state', 'should', 'only',\n",
    "'going', 'pointed', 'do', 'his', 'get', 'cannot', 'longest', 'during', 'him',\n",
    "'areas', 'h', 'she', 'x', 'where', 'we', 'see', 'are', 'best', 'said', 'ways',\n",
    "'away', 'enough', 'smallest',  'between', 'across', 'ends', 'never', 'opening',\n",
    "'however', 'come', 'both', 'c', 'last',  'many', 'against', 's', 'became',\n",
    "'faces', 'whole', 'asked', 'among', 'point', 'seems',  'furthered', 'furthers',\n",
    "'puts', 'three', 'been', 'much', 'interest', 'wants', 'worked',  'an',\n",
    "'present', 'case', 'myself', 'these', 'n', 'will', 'while', 'would', 'backing',\n",
    "'is', 'thus', 'them', 'someone', 'in', 'if', 'different', 'perhaps', 'things',\n",
    "'make',  'same', 'any', 'member', 'parts', 'several', 'higher', 'used', 'upon',\n",
    "'uses', 'thoughts',  'off', 'largely', 'i', 'well', 'anybody', 'finds',\n",
    "'thought', 'without', 'greatest',  'very', 'the', 'yours', 'latest', 'newest',\n",
    "'just', 'less', 'being', 'when', 'rooms',  'facts', 'yet', 'had', 'lets',\n",
    "'interested', 'has', 'gave', 'around', 'big', 'showing',  'possible', 'early',\n",
    "'know', 'like', 'necessary', 'd', 't', 'fully', 'become', 'works',  'grouping',\n",
    "'because', 'old', 'often', 'some', 'back', 'thinks', 'for', 'though', 'per',\n",
    "'everything', 'does', 'either', 'be', 'who', 'seconds', 'nowhere', 'although',\n",
    "'by', 'on',  'about', 'goods', 'asks', 'anything', 'of', 'o', 'or', 'into',\n",
    "'within', 'down', 'beings',  'right', 'your', 'her', 'area', 'downed', 'there',\n",
    "'long', 'way', 'was', 'opens', 'himself',  'but', 'newer', 'highest', 'with',\n",
    "'he', 'made', 'places', 'whether', 'j', 'up', 'us',  'problem', 'z', 'clear',\n",
    "'v', 'ordered', 'certain', 'general', 'as', 'at', 'face', 'again',  'no',\n",
    "'generally', 'backs', 'grouped', 'other', 'you', 'really', 'felt', 'problems',\n",
    "'important', 'sides', 'began', 'younger', 'e', 'longer', 'came', 'backed',\n",
    "'together',  'u', 'presenting', 'evenly', 'having', 'once'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_filt = modernism_word.map(str.lower)\\\n",
    "                                    .map(remove_punctuations)\\\n",
    "                                    .flatMap(str.split)\\\n",
    "                                    .filter(lambda word: word not in stopwords)\\\n",
    "                                    .filter(lambda word: len(word) > 3)\\\n",
    "                                    .filter(lambda word: word.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "modernism_word_count = modernism_word_filt.map(lambda x: (x, 1))\\\n",
    "                                          .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_count.top(10, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.C Learning: Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_vocab = modernism_word_count.keys().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_modern_vocab = sc.broadcast(modern_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_doc_bag = modernism_meta_text.values().map(lambda x: (x[0]['url'], x[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "\n",
    "mdwc_idx = modernism_doc_bag.mapValues(lambda words: list(filter(lambda word: word in br_modern_vocab.value, words)))\\\n",
    "                 .mapValues(lambda words: list(map(lambda word: br_modern_vocab.value[word], words)))\\\n",
    "                 .mapValues(Counter)\\\n",
    "                 .mapValues(lambda d: OrderedDict(sorted(d.items())))\\\n",
    "                 .mapValues(lambda counter: Vectors.sparse(len(br_modern_vocab.value), list(counter.keys()), list(counter.values())))\\\n",
    "                 .zipWithIndex().map(lambda x: [x[1], x[0][1]])\\\n",
    "                 .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 3\n",
    "ldaModel = LDA.train(mdwc_idx, k=numTopics, maxIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_top_vocab_inv = {v:k for k, v in modern_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for terms, termWeights in topicIndices:\n",
    "    print(\"TOPIC:\")\n",
    "    for term, weight in zip(terms, termWeights):\n",
    "        print(modern_top_vocab_inv[term], weight)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
