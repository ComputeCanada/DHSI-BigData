{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program Overview\n",
    "This script is part of a Hadoop map-reduce framework for reading through the OCR output of the London Times Digital Archive by Gale Cengage Learning and returns a heat map showing the relative frequency with which a give word appears on the front page. This is the map portion of the framework and is meant to be used with the accompanying reducer and final image producer.\n",
    "\n",
    "Originally produced to be used with a Big Data Analysis course at the Digital Humanities Summer Institute in 2016. Orignial idea by Pawel Pomorski.  Initial coding by John Simpson.\n",
    "\n",
    "# General Methodology\n",
    "This mapper leaves behind the ability to walk the XML trees of the OCR files in favour of the line by line processing required to use the streaming interface of Hadoop, which reads from standard in and writes to standard out.  Reading line by line, the mapper first uses a regular expression (regex) to find text that match the XML layout for returned words from the OCR output that additionally match the word that is being searched for.  When a match is made the groups of characters captured by the regex are extracted.  The numbers that set the grid location rows where the word appeared are used to set the range for the production of keys and the column numbers that set the grid location range are the values attached to those keys.  The key-value outputs are printed to the screen (standard out) where Hadoop will be able to grab them and pass them to the reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring this to be a Python Script\n",
    "While it is likely obvious within the Jupyter notebook environment in which this script was originally produced that this is a _Python_ script it will not necessarily be obvious to the other programs that will use this script in a production environment.  This first line tell those programs to use Python to interpret what follows.  Note that the first two characters are usually referred to as a \"shebang\" and they are used to do exactly this, make important declarations about how scripts and programs should be handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 2 to 3 compatibility\n",
    "\n",
    "Hadoop environments may use Python ranging from 2.6 to 3+.  To maximize compatibility we are using Python 2.6 printing syntax.  This first line of code ensures that it is parsed properly by Python 3+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "searchTerm = \"war\"\n",
    "searchTerm = searchTerm.lower()\n",
    "regEx = re.compile(r\"<wd pos=\\\"(\\d+,\\d+,\\d+,\\d+)\\\">([\\w\\'\\\",\\[\\]\\{\\}\\(\\)\\.]+)</wd>\")\n",
    "\n",
    "# will need RegEx since using STDIN and not lxml so won't have access to tree\n",
    "# General process:\n",
    "# 1. match line to <wd pos=\"(\\d+,\\d+,\\d+,\\d+)\">([\\w'\",\\[\\]\\(\\)\\{\\}\\.])+</wd>\n",
    "# 2. extract word group.\n",
    "# 3. convert word group to lowercase.\n",
    "# 4. if word group matches searchTerm then output the coordinates\n",
    "# 5. next line\n",
    "\n",
    "# input comes from STDIN (standard input), the equivalent of the keyboard or passing text\n",
    "for line in sys.stdin:\n",
    "    try:\n",
    "        # remove leading and trailing whitespace and end of line characters\n",
    "        line = line.strip(' \\n')\n",
    "        m = re.search(regEx, line)\n",
    "        if m != None:\n",
    "            #print(m.group(0),m.group(1),m.group(2))\n",
    "            if m.group(2).lower() == searchTerm:\n",
    "                # split m.group(1) at the commas\n",
    "                coords = [int(i) for i in m.group(1).split(',')] #nice use of list comprehension to convert to ints\n",
    "                # create a numpy array that is one row\n",
    "                for row in range(coords[0],coords[2]+1):\n",
    "                    print('%s\\t%s,%s') % (row, coords[1],coords[3])\n",
    "                # for each row, items [0] to [2], increment\n",
    "                # cells called out by items [1] to [3]\n",
    "                # print(m.group(1))\n",
    "                # print(coords, type(coords[0]))\n",
    "\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
