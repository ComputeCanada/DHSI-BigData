{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Interactive Big Data Analysis with Spark\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Curation]()\n",
    "2. [Preparation]()\n",
    "  1. [Data Importation](#2.A-Data-Importation)\n",
    "  2. [Package Installation](#2.B-Package-Installation)\n",
    "  3. [Package Importation](#2.C-Package-Importation)\n",
    "  4. [Context Creation](#2.D-Context-Creation)\n",
    "3. [Preprocessing]()\n",
    "  1. [Creating an RDD](#3.A-Creating-an-RDD)\n",
    "  2. [Getting Help](#3.B-Getting-Help)\n",
    "  3. [Action on a Dataset](#4.-Action-on-a-Dataset)\n",
    "  4. [Dataset Transformation](#5.-Dataset-Transformation)\n",
    "  5. [Filtering a Dataset](#7.-Filtering-a-Dataset)\n",
    "  6. [Caching a Dataset](#6.-Caching-a-Dataset)\n",
    "4. [Processing]()\n",
    "  1. []()\n",
    "  2. []()\n",
    "5. [Storage]()\n",
    "6. [Recap](#6.-Recap)\n",
    "7. [References](#7.-References)\n",
    "\n",
    "## List of Exercises\n",
    "1. [Exercise 1: How to create an RDD?](#Exercise-1)\n",
    "1. [Exercise 1: How to Count?](#Exercise-1)\n",
    "2. [Exercise 2: How to Transform?](#Exercise-2)\n",
    "3. [Exercise 3: How to Filter?](#Exercise-3)\n",
    "4. [Exercise 4: How to Sort?](#Exercise-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Curation\n",
    "\n",
    "From Wikipedia:\n",
    "> Data curation is a term used to indicate management activities related to organization and integration of data collected from various sources, annotation of the data, and publication and presentation of the data such that the value of the data is maintained over time, and the data remains available for reuse and preservation. Data curation includes \"all the processes needed for principled and controlled data creation, maintenance, and management, together with the capacity to add value to data\". In science, data curation may indicate the process of extraction of important information from scientific texts, such as research articles by experts, to be converted into an electronic format, such as an entry of a biological database. The term is also used in the humanities, where increasing cultural and scholarly data from digital humanities projects requires the expertise and analytical practices of data curation. In broad terms, curation means a range of activities and processes done to create, manage, maintain, and validate a component.\n",
    "\n",
    "> According to the University of Illinois' Graduate School of Library and Information Science, \"Data curation is the active and on-going management of data through its lifecycle of interest and usefulness to scholarship, science, and education; curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time.\"\n",
    "\n",
    "Curation being a field of its own, we will pass on the actual technics behind it. For this course, we will use a precurated dataset, the [eBooks@Adelaide dataset](https://ebooks.adelaide.edu.au/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparation\n",
    "\n",
    "### 2.A Data Importation\n",
    "In order for all nodes of our cluster to access our data, we have previously imported the data in HDFS. Here are the commands that were used to import the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```Shell\n",
    "hdfs dfs -mkdir /adelaide/\n",
    "hdfs dfs -mkdir /adelaide/meta\n",
    "hdfs dfs -mkdir /adelaide/page\n",
    "hdfs dfs -put ~/datasets/meta/*.json /adelaide/meta\n",
    "hdfs dfs -put ~/datasets/meta/*.json /adelaide/meta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the data is actually available by listing the content of the folders on HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /adelaide/page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /adelaide/meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B Python Package Installation\n",
    "\n",
    "To analyse our data, we will require some Python packages:  \n",
    "- numpy for numeric data manipulation;\n",
    "- networkx for network analysis;\n",
    "- plotly for plotting;\n",
    "- beautifulsoup4 to parse and extract data from HTML pages.\n",
    "\n",
    "These packages have been installed with the following command:\n",
    "```\n",
    "pip install numpy networkx plotly beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C Package Importation\n",
    "\n",
    "In this notebook, we will use [Apache Spark](http://spark.apache.org) to analyze brievly the Adelaide University's Book Dataset.\n",
    "\n",
    "First, we need to import Spark's Python module named `pyspark`. The module [`findspark`](https://github.com/minrk/findspark) is a wrapper that help us find the `pyspark`Â  module wherever it  is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import some Python standard modules that will help us during the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we import an interactive chart draing library [plotly](https://plot.ly/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode() # run at the start of every ipython notebook to use plotly.offline\n",
    "                     # this injects the plotly.js source files into the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.D Context Creation\n",
    "\n",
    "Once we have imported the required packages, we need to create a SparkContext. The context is an object that allows us to interact with the Spark cluster and create new resilient distributed dataset (RDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName(\"AdelaideNotebook\")\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except:\n",
    "    print(\"Warning : a SparkContext already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context read the configuration file of Spark and automatically deduces the configuration of our cluster.\n",
    "\n",
    "We can consult the dashboard of our Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "### 3.A Creating an RDD\n",
    "\n",
    "We will now create an RDD. It represents the books' meta information in [JSON](http://www.json.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adelaide_meta_json = sc.textFile('hdfs://hdp:9000/adelaide/meta/*.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an entry of the `adelaide_meta_json` RDD:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"@context\": \"http://schema.org\", \n",
    "  \"dateModified\": \"2014-02-26\", \n",
    "  \"image\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/cover.jpg\", \n",
    "  \"author\": \"Bowen, Marjorie, 1885-1952\", \n",
    "  \"@type\": \"Book\", \n",
    "  \"source\": \"https://gutenberg.net.au/ebooks09/0900581.txt\", \n",
    "  \"inLanguage\": \"en\", \n",
    "  \"publisher\": \"The University of Adelaide Library\", \n",
    "  \"name\": \"The Avenging of Ann Leete\", \n",
    "  \"keywords\": \"Literature\", \n",
    "  \"url\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/\", \n",
    "  \"description\": \"The Avenging of Ann Leete / Marjorie Bowen\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a few entries with the RDD's method `take` to get the first `K` elements of the meta information dataset. Here, `K = 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_first4 = adelaide_meta_json.take(4)\n",
    "print(meta_first4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `take` returns a list, we can iterate on the result and print it \"prettily\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for entry in meta_first4:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 : Create an RDD\n",
    "\n",
    "Create a new RDD named `adelaide_page_json` that contains the book URLs and its content as HTML code.\n",
    "\n",
    "The path to the page files in HDFS is `/adelaide/page/`.\n",
    "\n",
    "Here is an example of an entry of the `adelaide_page_json` RDD:\n",
    "```\n",
    "[\"https://ebooks.adelaide.edu.au/m/maupassant/guy/kiss/\", \"<!DOCTYPE html>\\n\\n<html> [...]\"]\n",
    "```\n",
    "\n",
    "Then retrieve the first element of that RDD and show it on screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adelaide_page_json = <FILL_IN>\n",
    "adelaide_page_json = sc.textFile('hdfs://hdp:9000/adelaide/page/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(adelaide_page_json.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B Getting Help\n",
    "\n",
    "At any moment, you can get help on a Python object using the `help()` function. For example, if we want to know more aboud the RDD's `take()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(adelaide_meta_json.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C Action on a Dataset\n",
    "\n",
    "The `take()` method is one among multiple available *actions* we apply on an RDD. An exhaustive list of action is available at the following URL:\n",
    "https://spark.apache.org/docs/latest/programming-guide.html#actions\n",
    "\n",
    "In case where we do not want to leave the notebook tab, we can call `help()` directly on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(adelaide_meta_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the available actions, there is method named `count()`.\n",
    "\n",
    "#### Exercise 2:  How to Count ?\n",
    "\n",
    "Call the help function on the count method of the `adelaide_meta_json` to get to know more about the `count()` action. Then, apply this action on both RDDs and print the result to screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# meta_count = adelaide_meta_json.count()\n",
    "# page_count = adelaide_page_json.count()\n",
    "# print(meta_count, page_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action apply on an RDD leads to the creation of one or many task and the production of a result. Every task executed in the same app can be visualised in the Spark's dashboard. In this interface, we can track the progress of a task, and check different performance measures on the task, for example its duration and cache statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.D Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the first 4 elements of our datasets that we retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_first4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We realize that the RDD is composed of by the lines of the input text files, but that is not possible to access to individual field in each dictionnary . **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_first = adelaide_meta_json.first()\n",
    "meta_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action `first()` as its name states, return the first entry of the dataset. We see that **each entry is a single string**. We will need to transform each entry of the RDD in order to convert the strings encoded in JSON in a Python dictionary. To do this, we will use the Python standard library function **`json.loads`** to convert each JSON encoded string into its Python equivalent.\n",
    "\n",
    "First, lets test it `json.loads` on the previous first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.loads(meta_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to apply this transformation to every RDD's entry. The RDD's method `map(func)` returns a new distributed dataset formed by passing each element of the source through a function *func*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adelaide_meta = adelaide_meta_json.map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of this transformation is *lazy*. Spark does not compute anything as long as a result is not requested by an action. To convince yourself, execute the preceding cell, then visit the Spark dashboard. You should see that no job have been added to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince ourselves that the transformation will be successfully applied, we can retrieve the first element of the transformed RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(adelaide_meta.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: How to Transform?\n",
    "\n",
    "Apply the JSON transformation on the page RDD that we have created in exercise 1 and print the URL of the fifth element of that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_page = adelaide_page_json.map(json.loads)\n",
    "print(adelaide_page.take(5)[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.E Filtering a Dataset\n",
    "\n",
    "#### 3.E.1 Filtering Bad Entries\n",
    "\n",
    "Since we now have RDDs that are easier to manipulate, we can start the analysis. \n",
    "\n",
    "Our dataset was built by scraping the webpages of Adelaide University. However, during the process, some of the webpages could be fetched by our spider pogram. Therefore, in our dataset, we end up with two kinds of entry.\n",
    "\n",
    "Good entry example:\n",
    "```\n",
    "{\"@context\": \"http://schema.org\", \"dateModified\": \"2014-02-26\", \"image\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/cover.jpg\", \"author\": \"Bowen, Marjorie, 1885-1952\", \"@type\": \"Book\", \"source\": \"https://gutenberg.net.au/ebooks09/0900581.txt\", \"inLanguage\": \"en\", \"publisher\": \"The University of Adelaide Library\", \"name\": \"The Avenging of Ann Leete\", \"keywords\": \"Literature\", \"url\": \"https://ebooks.adelaide.edu.au/b/bowen/marjorie/avenging-of-ann-leete/\", \"description\": \"The Avenging of Ann Leete / Marjorie Bowen\"}\n",
    "```\n",
    "\n",
    "Bad entry example:\n",
    "```\n",
    "{\"description\": \"ERROR_COMP_NOT_FOUND\"}\n",
    "```\n",
    "\n",
    "For the next operation, we wish to only keep entries for which we at least know the name of the author and the title of the book. To do so, we first define a function that returns `True` if the fields `author` and `name` are in the dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_author_title_defined(record):\n",
    "    return \"author\" and \"name\" in record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to answer the following quiz before executing the cell:  \n",
    "* What sort of argument takes the `filter()` method?\n",
    "* Is filter an action or a transformation?\n",
    "* What does `filter()` return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta_filt = adelaide_meta.filter(is_author_title_defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "For the next exercise, you will design your own bad entry filter for the books' page RDD. The page RDD's entries are not dictonaries but lists. Here is an example of a bad entry:\n",
    "```\n",
    "[\"https://ebooks.adelaide.edu.au/d/dante/\", \"None\"]\n",
    "```\n",
    "\n",
    "Write a function that will return `True` or `False` wether the entry is good or bad then create a new RDD named `adelaide_meta_filt` by applying your filter function of every entries of adelaide_page.\n",
    "\n",
    "To assess your filter design, count the number of elements in the resulting RDD. How many entries have you filtered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_page_filt = adelaide_page.filter(lambda rec: rec[1] != \"None\")\n",
    "adelaide_page_filt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.E.2 Filtering Duplicate Entries\n",
    "\n",
    "The meta-informations on each book have been recovered by scraping the website of [Adelaide University's eBook Libary](https://ebooks.adelaide.edu.au/). Since two pages could point to the same book, there is a possibility that a book is present more than once in our dataset.\n",
    "\n",
    "#### Exercise 5\n",
    "\n",
    "**To confirm that some books are present more than once in our dataset,  transform the dataset `adelaide_meta_filt` in a second that dataset that only includes URLs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_urls = adelaide_meta_filt.map(lambda rec: rec['url'])# (<FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs have a special method named `distinct()` that returns a new RDD containing strictly unique values. Next, we are going to call this method on our RDD of URLs and count the number of elements in that RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_urls.count() - meta_urls.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 305 duplicated entries in our dataset. To remove the duplicated entries, we will need to first associate an identifier that should be unique to each entry. We will refer to this id as a key. A unique identifier for a webpage is its URL. Since every book is associated to a URL, we will use this value as the key to our entries.\n",
    "\n",
    "The Spark RDD method `keyBy()` allows to associate each entry in our dataset with a key. The key will be defined as an item of the record, in this case the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta_url_key = adelaide_meta_filt.keyBy(lambda rec: rec['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry now has its own key, to convince ourselves, we can fetch the first element of that last RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta_url_key.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to keep only one value for each entry that shares the same key. To do this, we will apply a reduction operation, such that if we are given two records `a` and `b` with the same key, we only return the first record `a`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adelaide_meta_unique = adelaide_meta_url_key.reduceByKey(lambda a, b: a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This series of operations are called a reduction operation on a key-value pair RDD. We will get more into details in two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "The page dataset may also includes duplicated pages. Identify what should be the key of that dataset then try to create a new dataset with only unique book entries. What is the structure of an entry after applying the `keyBy()` method? Do we need to transform this dataset or could it be reduced directly?\n",
    "\n",
    "Count the number of elements in the resulting RDD to confirm your transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_page_unique = adelaide_page_filt.reduceByKey(lambda a, b: a)\n",
    "print(adelaide_page_unique.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.F Caching a Dataset\n",
    "\n",
    "When we expect to operate frequently on the same dataset, it can be useful to tell Spark to keep it in memory.\n",
    "\n",
    "To do so, we use the `cache()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta_unique.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDDs stored in memory are displayed in the **Storage** section of Spark web interface. Note that datasets are not loaded in memory until an action is called on them. \n",
    "\n",
    "Action on cached dataset are much faster than non cached dataset. But in order to be cached, an action must first be applied on the dataset. Based on that, try to explain the execution time for the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time adelaide_meta_unique.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% time adelaide_meta_unique.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Cache the RDD from exercise 6 and evaluate how long does it takes to retrieve the first 5 elements before and after caching. What happens when there is not enough memory to cache an RDD? Can you figure how to *uncache* an RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_page_unique.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% time first_5pages = adelaide_page_unique.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% time first_5pages = adelaide_page_unique.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grasp the extent of our dataset, we can first count to number of entries it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry is of our dataset is a dictionary. Each dictionary can contain a variable number of keys and each dictionary in our dataset do not necessarily shared the same keys.\n",
    "\n",
    "We can extract the keys from each dictionary and count how many times they are present. To access, the key of a dictionary, we can use the method `keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta.first().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to apply this method to every dictionary in our dataset, so that would be a map. However, if we simply apply a map, we will get an RDD of key-lists. What we truly want is to merge the list to get an RDD of keys. \n",
    "\n",
    "Spark has a function to merge the iterable return by a function, the `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_keys = adelaide_meta.flatMap(lambda rec: rec.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first 5 elements of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_keys.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in finding the frequency of each key. This will give us an idea of the completeness of our dataset. To compute the key frequency, we will need to apply a classic map-reduce pattern.\n",
    "\n",
    "First, we need to pair each key with the basic frequency value 1. This is the map operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_key_value = adelaide_keys.map(lambda key: (key, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can inspect the result of the transformation by looking at the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_key_value.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will do next is add the value associated with each pair that shares the same key. This is the reduce operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "adelaide_agg = adelaide_key_value.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can collect our transformed RDD. Key-Value pair RDDs have a special method `collectAsMap` that returns the result as a dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_agg.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that only a few keys are available in most dictionaries of our datasets. We should therefore restrict our analysis to these fields or create new fields from the frequent one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.A Valorizing data by transforming the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_name_birth_death(record):\n",
    "    author = record.get('author', None)\n",
    "    if author:\n",
    "        author = author.strip()\n",
    "        # Remove trailing dot\n",
    "        if '.' == author[-1]:\n",
    "            author = author[:-1]\n",
    "        try:\n",
    "            lastname, firstname, birth_death = author.split(',')\n",
    "        except ValueError:\n",
    "            return record\n",
    "        try:\n",
    "            birth, death = re.findall('\\d+', birth_death)\n",
    "        except ValueError:\n",
    "            return record\n",
    "        record['author_lastname'] = lastname.strip()\n",
    "        record['author_firstname'] = firstname.strip()\n",
    "        record['author_birth'] = int(birth)\n",
    "        record['author_death'] = int(death)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta_val = adelaide_meta_unique.mapValues(process_name_birth_death).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_meta_val.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_dateCreated(record):\n",
    "    if 'dateCreated' in record:\n",
    "        dates = re.findall('\\d+', record['dateCreated'])\n",
    "        if len(dates) > 0:\n",
    "            date = int(dates[0])\n",
    "            # Check if the date is before common era\n",
    "            if re.match(r'BC|bc|BCE|bce', record['dateCreated']):\n",
    "                date *= -1\n",
    "            record['dateCreated'] = date\n",
    "        else:\n",
    "            del record['dateCreated']\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adelaide_meta_val = adelaide_meta_val.map(convert_dateCreated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.B First analysis: authors' life expectancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_age(rec):\n",
    "    \"\"\"Compute the age of an author when it died\n",
    "    based on its year of birth and death.\n",
    "    \"\"\"\n",
    "    if 'author_birth' and 'author_death' in rec:\n",
    "        birth, death =  rec['author_birth'], rec['author_death']\n",
    "        if birth < death:\n",
    "            return death - birth\n",
    "        else:\n",
    "            # If year of birth is greater than year of death the\n",
    "            # author was born in BCE. Do you think this is correct\n",
    "            # in every cases?\n",
    "            return birth - death\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "age_frequency = adelaide_meta_val.map(compute_age)\\\n",
    "                                 .countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization can be done with multiple tools, here we use plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Bar(\n",
    "        x=list(age_frequency.keys()),\n",
    "        y=list(age_frequency.values()),\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Adelaide authors life expectancy\",\n",
    "    xaxis=dict(\n",
    "        title='life expectancy (years)',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='number of authors',\n",
    "    ),    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A surprising number of authors died at the age of 43. There is either a pattern with authors, or we have commited a mystake in our analysis.\n",
    "\n",
    "What happens if an author has written more than one book? We need to remember that our dataset is composed of books, not authors. If we want to produce statistics on the authors, we need to keep only distinct authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.C Second analysis: authors' life expectancy... done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_name_age(rec):\n",
    "    age = compute_age(rec)\n",
    "    lastname = rec['author_lastname']\n",
    "    firstname = rec['author_firstname']\n",
    "    return firstname, lastname, age\n",
    "\n",
    "authors = adelaide_meta_val.filter(lambda rec: 'author_lastname' in rec)\\\n",
    "                           .map(retrieve_name_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_authors = authors.distinct()\n",
    "age_frequency2 = unique_authors.map(lambda tup: tup[2]).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Bar(\n",
    "        x=list(age_frequency2.keys()),\n",
    "        y=list(age_frequency2.values()),\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Adelaide authors life expectancy\",\n",
    "    xaxis=dict(\n",
    "        title='life expectancy (years)',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='number of authors',\n",
    "    ),    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Storage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recap\n",
    "### Preprocessing the pages to extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url, book1 = adelaide_page.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_page_unique = adelaide_page.groupByKey()\\\n",
    "                                    .mapValues(list)\\\n",
    "                                    .mapValues(itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_text(page):\n",
    "    if page:\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        it = chain(soup.findAll(['meta', 'script', 'head']),\n",
    "                   soup.findAll('div', {\"id\" : \"controls\"}),\n",
    "                   soup.findAll('div', {\"class\" : \"contents\"}),\n",
    "                   soup.findAll('div', {\"class\" : \"titleverso\"}),\n",
    "                   soup.findAll('div', {\"class\" : \"colophon\"}),\n",
    "                   soup.findAll('span', {\"class\" : \"author\"}))\n",
    "        for div in it:\n",
    "            div.extract()\n",
    "        return soup.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_text = adelaide_page_unique.mapValues(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adelaide_text.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing: Analysing the work of an era\n",
    "\n",
    "We have an dataset of 4371 different books written at different times. Lets suppose we want to study the texts of the books written during the 1901â1939 Modernism era.\n",
    "\n",
    "First we need to identify which books in our dataset were written during this era."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modern_era_books = adelaide_meta_val.filter(lambda rec: 1900 < rec.get('dateCreated', 0) < 1938)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these books are not necessarily in English. We therefore need to apply a second filter on the language (`inLanguage`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modern_era_en_books = modern_era_books.filter(lambda rec: rec.get('inLanguage', '') == 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now count how many English books from our dataset were written during this era."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modern_era_en_books.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the number of distinct authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modern_era_books.map(lambda rec: rec['author']).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meta information about each book and the book's text are stored in two separate RDDs. In order to retrieve the texts written during the modernism era, we will need to join the RDD of modern book era metainformation and the RDD of books' text.\n",
    "\n",
    "To do so, we will first need to define the modern era book RDD as an RDD of key-value pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modern_era_books_kv = modern_era_books.keyBy(lambda rec: rec['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modern_era_books_kv.keys().distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can join the RDD of meta information on modern era books with the RDD of books' text to access the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modernism_meta_text = modern_era_books_kv.join(adelaide_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we join the words of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modernism_text = modernism_meta_text.mapValues(lambda x: x[1])\n",
    "modernism_word = modernism_text.flatMapValues(string.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "def remove_punctuations(word):\n",
    "    return re.sub(r'[{}âââââ]'.format(punctuation), \" \", word).strip()\n",
    "\n",
    "stopwords  = set(['all', 'pointing', 'four', 'go', 'oldest', 'seemed', 'whose', 'certainly',\n",
    "'young',  'presents', 'to', 'asking', 'those', 'under', 'far', 'every',\n",
    "'presented', 'did',  'turns', 'large', 'p', 'small', 'parted', 'smaller',\n",
    "'says', 'second', 'further',  'even', 'what', 'anywhere', 'above', 'new',\n",
    "'ever', 'full', 'men', 'here', 'youngest',  'let', 'groups', 'others', 'alone',\n",
    "'along', 'great', 'k', 'put', 'everybody', 'use',  'from', 'working', 'two',\n",
    "'next', 'almost', 'therefore', 'taken', 'until', 'today',  'more', 'knows',\n",
    "'clearly', 'becomes', 'it', 'downing', 'everywhere', 'known', 'cases',  'must',\n",
    "'me', 'states', 'room', 'f', 'this', 'work', 'itself', 'can', 'mr', 'making',\n",
    "'my', 'numbers', 'give', 'high', 'something', 'want', 'needs', 'end', 'turn',\n",
    "'rather', 'how', 'y', 'may', 'after', 'such', 'man', 'a', 'q', 'so', 'keeps',\n",
    "'order', 'furthering',  'over', 'years', 'ended', 'through', 'still', 'its',\n",
    "'before', 'group', 'somewhere',  'interesting', 'better', 'differently',\n",
    "'might', 'then', 'non', 'good', 'somebody',  'greater', 'downs', 'they', 'not',\n",
    "'now', 'gets', 'always', 'l', 'each', 'went', 'side',  'everyone', 'year',\n",
    "'our', 'out', 'opened', 'since', 'got', 'shows', 'turning', 'differ',  'quite',\n",
    "'members', 'ask', 'wanted', 'g', 'could', 'needing', 'keep', 'thing', 'place',\n",
    "'w', 'think', 'first', 'already', 'seeming', 'number', 'one', 'done',\n",
    "'another', 'open',  'given', 'needed', 'ordering', 'least', 'anyone', 'their',\n",
    "'too', 'gives', 'interests',  'mostly', 'behind', 'nobody', 'took', 'part',\n",
    "'herself', 'than', 'kind', 'b', 'showed',  'older', 'likely', 'r', 'were',\n",
    "'toward', 'and', 'sees', 'turned', 'few', 'say', 'have',  'need', 'seem',\n",
    "'saw', 'orders', 'that', 'also', 'take', 'which', 'wanting', 'sure', 'shall',\n",
    "'knew', 'wells', 'most', 'nothing', 'why', 'parting', 'noone', 'later', 'm',\n",
    "'mrs', 'points', 'fact', 'show', 'ending', 'find', 'state', 'should', 'only',\n",
    "'going', 'pointed', 'do', 'his', 'get', 'cannot', 'longest', 'during', 'him',\n",
    "'areas', 'h', 'she', 'x', 'where', 'we', 'see', 'are', 'best', 'said', 'ways',\n",
    "'away', 'enough', 'smallest',  'between', 'across', 'ends', 'never', 'opening',\n",
    "'however', 'come', 'both', 'c', 'last',  'many', 'against', 's', 'became',\n",
    "'faces', 'whole', 'asked', 'among', 'point', 'seems',  'furthered', 'furthers',\n",
    "'puts', 'three', 'been', 'much', 'interest', 'wants', 'worked',  'an',\n",
    "'present', 'case', 'myself', 'these', 'n', 'will', 'while', 'would', 'backing',\n",
    "'is', 'thus', 'them', 'someone', 'in', 'if', 'different', 'perhaps', 'things',\n",
    "'make',  'same', 'any', 'member', 'parts', 'several', 'higher', 'used', 'upon',\n",
    "'uses', 'thoughts',  'off', 'largely', 'i', 'well', 'anybody', 'finds',\n",
    "'thought', 'without', 'greatest',  'very', 'the', 'yours', 'latest', 'newest',\n",
    "'just', 'less', 'being', 'when', 'rooms',  'facts', 'yet', 'had', 'lets',\n",
    "'interested', 'has', 'gave', 'around', 'big', 'showing',  'possible', 'early',\n",
    "'know', 'like', 'necessary', 'd', 't', 'fully', 'become', 'works',  'grouping',\n",
    "'because', 'old', 'often', 'some', 'back', 'thinks', 'for', 'though', 'per',\n",
    "'everything', 'does', 'either', 'be', 'who', 'seconds', 'nowhere', 'although',\n",
    "'by', 'on',  'about', 'goods', 'asks', 'anything', 'of', 'o', 'or', 'into',\n",
    "'within', 'down', 'beings',  'right', 'your', 'her', 'area', 'downed', 'there',\n",
    "'long', 'way', 'was', 'opens', 'himself',  'but', 'newer', 'highest', 'with',\n",
    "'he', 'made', 'places', 'whether', 'j', 'up', 'us',  'problem', 'z', 'clear',\n",
    "'v', 'ordered', 'certain', 'general', 'as', 'at', 'face', 'again',  'no',\n",
    "'generally', 'backs', 'grouped', 'other', 'you', 'really', 'felt', 'problems',\n",
    "'important', 'sides', 'began', 'younger', 'e', 'longer', 'came', 'backed',\n",
    "'together',  'u', 'presenting', 'evenly', 'having', 'once'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modernism_word_filt = modernism_word.mapValues(string.lower)\\\n",
    "                                    .mapValues(remove_punctuations)\\\n",
    "                                    .flatMapValues(string.split)\\\n",
    "                                    .filter(lambda pair: pair[1] not in stopwords)\\\n",
    "                                    .filter(lambda pair: len(pair[1]) > 3)\\\n",
    "                                    .filter(lambda pair: pair[1].isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "modernism_word_count = modernism_word_filt.values()\\\n",
    "                                          .map(lambda x: (x, 1))\\\n",
    "                                          .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modernism_word.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modernism_word_count.top(10, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced processing : Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# modern_top_vocab = list(map(lambda x: x[0], modernism_word_count.top(1000, key=lambda x: x[1])))\n",
    "modern_top_vocab = modernism_word_count.keys().collect()\n",
    "modern_vocab = dict(list(zip(modern_top_vocab, range(len(modern_top_vocab)))))\n",
    "VEC_LENGTH = len(modern_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "br_modern_vocab = sc.broadcast(modern_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from operator import add\n",
    "import numpy as np\n",
    "\n",
    "def createCombiner(word):\n",
    "    if word in br_modern_vocab.value:\n",
    "        idx = br_modern_vocab.value[word]\n",
    "        return Vectors.dense([0] * idx + [1] * (VEC_LENGTH - idx))\n",
    "    else:\n",
    "        return Vectors.dense([0] * VEC_LENGTH)\n",
    "\n",
    "def mergeValue(vector, word):\n",
    "    return vector + createCombiner(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modernism_doc_word_count = modernism_word_filt.combineByKey(createCombiner, mergeValue, add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdwc_idx = modernism_doc_word_count.map(lambda x: [hash(x[0]), x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numTopics = 10\n",
    "ldaModel = LDA().train(mdwc_idx, k=numTopics, maxIterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for terms, termWeights in topicIndices:\n",
    "    print(\"TOPIC:\")\n",
    "    for term, weight in zip(terms, termWeights):\n",
    "        print(modern_top_vocab[term], weight)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
