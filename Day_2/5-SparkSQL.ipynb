{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Structured Data Analysis with Spark SQL\n",
    "\n",
    "Big Data is not only raw text files, a lot of datasets available are structured as table and even raw text files have a underlying structure. Spark SQL allow to query structured datasets, relational database and also provide an API to structure data.\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "1. [Initialization](#1.-Initialization)  \n",
    "  1.1 [Spark](#1.1-Spark)  \n",
    "  1.2 [Spark SQL](#1.2-Spark-SQL)\n",
    "2. [Dataframe](#2.-Dataframe)  \n",
    "  2.1 [Reading Data](#2.1-Reading-Data)  \n",
    "  2.2 [Structuring Data](#2.2-Structuring-Data)  \n",
    "  2.3 [Creating a Dataframe](#2.3-Creating-a-Dataframe)  \n",
    "  2.4 [Registering a Table](#2.4-Registering-a-Table)  \n",
    "  2.5 [Querying Data](#2.5-Querying-Data)  \n",
    "  2.6 [Aggregating Results](#2.6-Aggregating-Results)\n",
    "3. [Writing Results to Disk](#3.-Writing-Results-to-Disk)  \n",
    "  3.1 [Apache Parquet](#3.1-Apache-Parquet)  \n",
    "  3.2 [Other Formats](#3.2-Other-Formats)    \n",
    "4. [Reading the Dataframe from Storage](#4.-Reading-the-Dataframe-from-Storage)  \n",
    "  4.1 [Reading CSV Files](#4.1-Reading-CSV-Files)\n",
    "5. [Manipulating Data in a DataFrame](#5.-Manipulating-Data-in-DataFrames)\n",
    "4. [Ending Spark SQL Analysis](#6.-Ending-Spark-SQL-Analysis)\n",
    "6. [Recap](#7.-Recap)\n",
    "7. [References](#References)\n",
    "\n",
    "## List of Exercises\n",
    "\n",
    "1. [Exercise 1: Initialize Spark](#Exercise-1)\n",
    "2. [Exercise 2: Create an RDD](#Exercise-2)\n",
    "3. [Exercise 3: Count Elements](#Exercise-3)\n",
    "4. [Exercise 4: Transform an RDD](#Exercise-4)\n",
    "5. [Exercise 5: Validate Transformation](#Exercise-5)\n",
    "6. [Exercise 6: Type RDD fields](#Exercise-6)\n",
    "7. [Exercise 7: Return on key-value notions](#Exercise-7)\n",
    "7. [Exercise 8: Create a Dataframe](#Exercise-8)\n",
    "8. [Exercise 9: Display a Dataframe](#Exercise-9)\n",
    "9. [Exercise 10: Create a Dataframe from a CSV](#Exercise-10)\n",
    "9. [Exercise 11: Print the Dataframe's schema](#Exercise-11)\n",
    "9. [Exercise 12: Aggregate the core-hours per PI](#Exercise-12)\n",
    "9. [Exercise 13: End the Context](#Exercise-13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Spark\n",
    "#### Exercise 1\n",
    "\n",
    "Import the necessary Python module(s) and create a Spark context. \n",
    "\n",
    "**Warning**, verify if there exists a context and handle possible exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Spark SQL\n",
    "\n",
    "We can now import the components that we need to analyze structured data from Spark SQL module `pyspark.sql`.\n",
    "* `SQLContext`: Main entry point for Spark SQL functionality. It will be used to create Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark SQL Context requires a Spark Context as sole argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Dataframe\n",
    "\n",
    "Textbook definition:    \n",
    "\n",
    "> A data frame is a table, or two-dimensional array-like structure, in which each column contains measurements on one variable, and each row contains one case.\n",
    "\n",
    "In Spark, a dataframe is a distributed collection of data grouped into named columns. It is equivalent to a relational table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1 Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Create an RDD with the dataset found in `/scratch/formation/spark/data/pagecounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Count the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Transform the previous RDD into a second one where each field originally separated by white spaces are now elements of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "To validate the transformation, display the first 8 elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 6\n",
    "\n",
    "As you can see, the third and fourth fields are numbers represented as text. Transform the RDD in order to convert these strings to `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Structuring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our original dataset is strictly text, we now want to give a structure that is define field name and type.\n",
    "\n",
    "\n",
    "A pagecounts file looks like this\n",
    "```\n",
    "af Spesiaal:Onlangse_wysigings 3 101681\n",
    "af Spesiaal:RecentChanges 2 2248\n",
    "af Suid-Afrika 1 30698\n",
    "af Tuisblad 14 155257 \n",
    "af Varkgriep 4 42236\n",
    "af Wikipedia 2 32796\n",
    "```\n",
    "\n",
    "It is a tabular file, where each line is a distinct entry and the columns represent\n",
    "\n",
    "1. the project name (language);\n",
    "2. the page title;\n",
    "3. the number of requests;\n",
    "4. the size of the content returned.\n",
    "\n",
    "To define our structure, we use Spark SQL data types that are defined in [`pyspark.sql.types`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types). We import a small subset of type that we need `LongType` and `StringType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark SQL also provides two types to defines dataframe structure:\n",
    "- `StructType`: Data type representing a row of a dataframe.\n",
    "- `StructField`: Data type representing a field of a row. It is mainly defined by a name and a type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using all these classes, we can define our data schema. The order in the list must correspond to the order in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('lang',    StringType()), \n",
    "                     StructField('name',    StringType()), \n",
    "                     StructField('request', LongType()), \n",
    "                     StructField('size',    LongType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 Creating a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To create a dataframe, we simply need to invoke the `createDataFrame()` method of our Spark SQL Context and provide it an RDD and our data structure (or schema). \n",
    "\n",
    "#### Exercise 8\n",
    "\n",
    "Replace `<FILL IN>` in the following cell by the proper RDD for `data/pagecounts`, and the second `<FILL IN>` by the method call to persist the dataframe in memory. **Hint**: it is the same method for RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts = sqlContext.createDataFrame(<FILL IN>, schema).<FILL IN>()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can then manipulate the dataframe using its [`pyspark.sql.DataFrame` API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "\n",
    "We can, for example, show the first lines of the dataframe as a table in ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.4 Registering a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to query our dataframe with SQL, we need to register it as a table in the Spark SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.registerTempTable(\"page_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.5 Querying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now able to interogate our data using the Structured Query Language (SQL). The following query request the top 10 most requested page in spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.sql(\"SELECT name, request \"\n",
    "                    \"FROM page_table \"\n",
    "                    \"WHERE lang='es' \"\n",
    "                    \"ORDER BY request DESC \"\n",
    "                    \"LIMIT 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A query on a dataframe is a Spark transformation. Therefore, to compute the result, we need to call an action. \n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "Call the right method to display the dataframe resulting from the preceding query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.5.1 SQL 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can decompose the preceding query in keywords:\n",
    "\n",
    "#### SELECT\n",
    "\n",
    "Indicate which variable we want to collect. The name of variable have been defined when structuring our data in [section 2.2](#2.2-Structuring-Data).\n",
    "\n",
    "#### FROM\n",
    "\n",
    "Indicate the source of data. The name of the table has been defined in [section 2.4](#2.4-Registering-a-table).\n",
    "\n",
    "#### WHERE\n",
    "\n",
    "Filter the entries based on predicates in function of the variables. \n",
    "\n",
    "#### ORDER BY [...] DESC\n",
    "\n",
    "Indicate we wish to order the resulting dataframe in function of a certain variable, in a certain order. \n",
    "\n",
    "#### LIMIT N \n",
    "\n",
    "Return only a subset of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.5.2 SQL as an API\n",
    "\n",
    "[Dataframe API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) includes methods that are named after SQL keywords. These methods can be used instead of the query language. However, the order does not match exactly as the query is a single statement, the API will return a dataframe after each method call. These calls can be chained to build a similar pipeline.\n",
    "\n",
    "The following example extract and show the 10 most requested pages in English on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.where(\"lang = 'en'\")\\\n",
    "            .select(\"name\", \"request\")\\\n",
    "            .orderBy(\"request\", ascending=False)\\\n",
    "            .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.6 Aggregating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we look back at the preceding results, we realise that some names appear multiple time. The reason is that we have omitted to sum the number of requests per page. We need to aggregate the page entries by name and sum the requests.\n",
    "\n",
    "To do this, we indicate in our query to sum the request (`SUM(request)`) for entries grouped by name (`GROUP BY NAME`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"SELECT name, SUM(request)\"\n",
    "               \"FROM page_table \"\n",
    "               \"WHERE lang='en' \"\n",
    "               \"GROUP BY name \"\n",
    "               \"ORDER BY SUM(request) DESC \"\n",
    "               \"LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Aggregating Using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.where(\"lang = 'en'\")\\\n",
    "            .select(\"name\", \"request\")\\\n",
    "            .groupBy('name')\\\n",
    "            .agg({'request' : 'sum'})\\\n",
    "            .orderBy(\"sum(request)\", ascending=False)\\\n",
    "            .limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Writing Results to Disk\n",
    "\n",
    "To avoid structuring our data each time, we can save the resulting dataframe to disk. This will preserve the schema and the data order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1 Apache Parquet\n",
    "\n",
    "[Apache Parquet](https://parquet.apache.org/) is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language. The format is fairly popular with the Spark community as it is efficient and easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.write.parquet(\"pagecounts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2 Other Formats\n",
    "\n",
    "Dataframe can also be written as JSON file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfPageCounts.write.json('pagecounts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It can also save the dataframe as a table in [Apache Hive](http://hive.apache.org/) or a database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Reading the Dataframe from Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A new Dataframe can be created by reading back from a file in one of the format we mentionned.\n",
    "\n",
    "For example, to create a dataframe from a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pagecount_parq = sqlContext.read.parquet(\"pagecounts.parquet\")\n",
    "pagecount_parq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 Reading CSV Files\n",
    "\n",
    "Datasets are often found as CSV files. \n",
    "\n",
    "Spark 2.0, which you are currently using, has direct support for CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read\\\n",
    "               .csv(path='/scratch/formation/spark/data/store/employees.tsv',\n",
    "                    header=True, inferSchema=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once the dataframe is loaded, we can verify that the columns were correctly named and their type correctly infered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If we are satisfied with the schema, we can start manipulating our dataframe or show its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5. Manipulating Data in DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To demonstrate how data in a dataframe can be manipulated with Spark, we are going to pursue this lesson with a more complex dataset. We are going to create a dataframe using our dataset of Moab log from lesson 2.\n",
    "\n",
    "Here are the informations regarding this dataset:  \n",
    "- path: '/scratch/formation/spark/data/moab/raw/*';\n",
    "- the csv does not have a column header;\n",
    "- each field is separated by a space;\n",
    "- we would like Spark to infer the schema.\n",
    "\n",
    "#### Exercise 10\n",
    "\n",
    "Based on what you learned in 4.1 and the information above, create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 11\n",
    "\n",
    "Look at the schema inferred by Spark, does it correspond to the one we declared in lesson 2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Often, the data source will be malformed somehow. It will either include corrupted entries which format is unexpected. This is the case of Moab log data. Some lines do not correspond to job events and therefore do not have the right format. Spark allows through an argument of the csv method to indicate which behaviour we want to adopt with malformed entries.\n",
    "\n",
    "The argument is named `mode` and can take three values `PERMISSIVE`, `DROPMALFORMED` and `FAILFAST`. We will experiment each behavior ourselves next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**PERMISSIVE mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqlContext.read.csv(path='/scratch/formation/spark/data/moab/raw/*', \n",
    "                    header=False,\n",
    "                    sep=' ',\n",
    "                    inferSchema=True,\n",
    "                    mode='PERMISSIVE').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**DROPMALFORMED mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqlContext.read.csv(path='/scratch/formation/spark/data/moab/raw/*', \n",
    "                    header=False,\n",
    "                    sep=' ',\n",
    "                    inferSchema=True,\n",
    "                    mode='DROPMALFORMED').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**FAILFAST mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sqlContext.read.csv(path='/scratch/formation/spark/data/moab/raw/*', \n",
    "                    header=False,\n",
    "                    sep=' ',\n",
    "                    inferSchema=True,\n",
    "                    mode='FAILFAST').show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Based on the format of our data, Spark can sometime have a hardtime inferring the right schema. Furthermore, when our dataset do not have an header line, we will need to specify the column name manually. This can all be done using `StructType`Â  and `StructField` as we have seen previously.\n",
    "\n",
    "In the next cell we define the column names and associated to each a column a type. This schema will then be used when calling the `read.csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "columns = ['event_time', 'event_epoch', 'event_type', 'job_id', 'job_event',\n",
    "           'n_nodes', 'n_cores', 'username', 'walltime',\n",
    "           'status', 'queue', 'submit_time', 'dispatch_time', 'start_time',\n",
    "           'end_time', 'project_id']\n",
    "types = [StringType(), StringType(), StringType(), IntegerType(), StringType(),\n",
    "         IntegerType(), IntegerType(), StringType(), IntegerType(),\n",
    "         StringType(), StringType(), IntegerType(), IntegerType(), IntegerType(),\n",
    "         IntegerType(), StringType()]\n",
    "schema = StructType([StructField(name, type_) for name, type_ in zip(columns, types)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we know our schema, we will instruct Spark to drop any line that do not fit it. This will have for effect to keep only the job event entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(path='/scratch/formation/spark/data/moab/raw/*', \n",
    "                    header=False,\n",
    "                    sep=' ',\n",
    "                    schema=schema,\n",
    "                    mode='DROPMALFORMED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.1 Enhancing our dataset by derivating columns\n",
    "\n",
    "Once we are guaranteed that each entries in the dataset respect our schema, we can transforms the data and derivate column. Like RDD, Spark's DataFrame are immutable. Therefore, if we wish to add a column or drop one, we will need to create a new DataFrame.\n",
    "\n",
    "The following cell computes a column containing the core-hours metric per job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('core_hours', (df['end_time'] - df['start_time']) / 3600. * df['n_cores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The core-hours metric do not make sense if the job is not completed. We would therefore prefer to only compute it on entries which status is `Completed`. To do this, we will create a Spark *User Defined Function* or *UDF*. We start by defining a function which take each column required by the computation in input and return the core-hours if the status is \"completed\". Otherwise the function returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def completed_core_hours(start_time, end_time, n_cores, status):\n",
    "    if status == 'Completed':\n",
    "        return (end_time - start_time) / 3600. * n_cores\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We then define the UDF using Spark's function `udf` which takes in argument a function and the type of the objects that will be returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "u_completed_core_hours = pyspark.sql.functions.udf(completed_core_hours, pyspark.sql.types.DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can then apply this function to each entry and add the result as a new column in a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('core_hours', u_completed_core_hours(df['start_time'],\n",
    "                                                         df['end_time'],\n",
    "                                                         df['n_cores'],\n",
    "                                                         df['status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As we can see in the next cell, only the entries where the job was completed had their core-hours metric computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df2.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While easy to use, UDF should be used as a last resort to transform columns. Spark SQL proposes a wide list of functions to transforms columns. Find out more [here](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercise 12\n",
    "\n",
    "Compute the total core-hours per PI. A PI is represented by the 7 first characters of `project_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6. Ending Spark SQL Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spark SQL's context do not need to be terminated prior to leaving the notebook.\n",
    "\n",
    "#### Exercise 13\n",
    "\n",
    "Terminate the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7. Recap\n",
    "\n",
    "In this notebook, we put in practice and learned about the following parts of \n",
    "**[Python Spark SQL API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)**:\n",
    "1. Import Spark SQL Python module: \n",
    "**[`import pyspark.sql`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)**\n",
    "2. Create a Spark SQLContext:\n",
    "**[`pyspark.sql.SQLContext()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext)**\n",
    "3. Create an RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "4. Count the number of elements in a RDD: \n",
    "**[`Rdd.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count)**\n",
    "5. Apply a transformation on each element of a RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "6. Take a the first *num* elements from an RDD: \n",
    "**[`Rdd.take(num)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take)**\n",
    "7. Structure and type data fields: **[`pyspark.sql.types`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types)**\n",
    "8. Create a dataframe from an RDD: **[`SQLContext.createDataFrame(RDD)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.createDataFrame)**\n",
    "9. Print the first *n* rows of a dataframe: **[`Dataframe.show(n)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)**\n",
    "10. Register a dataframe as a temporary table: **[`DataFrame.registerTempTable(name)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable)**\n",
    "11. Query a context's registered table: **[`SQLContext.sql(name)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql)**\n",
    "12. Use the SQL API of a dataframe: **[`DataFrame`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)**\n",
    "13. Write a dataframe as Parquet file: **[`DataFrame.write.parquet(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.parquet)**\n",
    "14. Write a dataframe as JSON file: **[`DataFrame.write.json(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.json)**\n",
    "15. Read a dataframe from a Parquet file: **[`DataFrame.read.parquet(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.parquet)**  \n",
    "16. Read a dataframe from a CSV file: **[`DataFrame.read.csv(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.csv)**\n",
    "10. End the SparkContext:\n",
    "**[`SparkContext.stop()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.stop)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8. References\n",
    "\n",
    "* [Berkeley AmpCamp 5 - Data Exploration Using Spark SQL](http://ampcamp.berkeley.edu/5/exercises/data-exploration-using-spark-sql.html)\n",
    "* [edX - Introduction to Big Data with Apache Spark](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)\n",
    "* [edX - Introduction to Big Data with Apache Spark (Github repo)](https://github.com/spark-mooc/mooc-setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (CQ-Spark)",
   "language": "python",
   "name": "spark-cq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
