{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Interactive Analysis with Spark\n",
    "\n",
    "## Table of Content\n",
    "1. [Initialization](#1.-Initialization)\n",
    "2. [Creating an RDD](#2.-Creating-an-RDD)\n",
    "3. [Getting Help](#3.-Getting-Help)\n",
    "4. [Action on a Dataset](#4.-Action-on-a-Dataset)\n",
    "5. [Dataset Transformation](#5.-Dataset-Transformation)\n",
    "6. [Caching a Dataset](#6.-Caching-a-Dataset)\n",
    "7. [Filtering a Dataset](#7.-Filtering-a-Dataset)\n",
    "8. [Reduction Operation](#8.-Reduction-Operation)  \n",
    "  8.1 [Filtering Unrelated Entries](#8.1-Filtering-Unrelated-Entries)  \n",
    "  8.2 [Transforming in Key-Value Pairs](#8.2-Transforming-in-Key-Value-Pairs)  \n",
    "  8.3 [Aggregating the Results by Key](#8.3-Aggregating-the-Results-by-Key)  \n",
    "  8.4 [Finding the Most Processing Hungry Projects](#8.4-Finding-the-Most-Processing-Hungry-Projects)    \n",
    "  8.5 [Bar Chart](#8.5-Bar-Chart)\n",
    "9. [Mini-Project](#9.-Mini-Project)\n",
    "10. [Ending the Analysis](#10.-Ending-the-Analysis)\n",
    "11. [Recap](#11.-Recap)\n",
    "12. [References](#12.-References)\n",
    "\n",
    "## List of Exercises\n",
    "1. [Exercise 1: How to Cache?](#Exercise-1)\n",
    "3. [Exercise 2: How to Filter?](#Exercise-2)\n",
    "2. [Exercise 3: How to Peek?](#Exercise-3)\n",
    "4. [Exercise 4: How to Sort?](#Exercise-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "\n",
    "In this notebook, we will use Spark to analyze a structured dataset.\n",
    "\n",
    "First, we need to import Spark's Python module named `pyspark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = pyspark.SparkContext()\n",
    "except ValueError:\n",
    "    print(\"Warning : a SparkContext already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we execute Spark locally, the context creation can be used to access Spark functions. It is also possible to launch Spark and the Python interpreter / notebook using the script named `pyspark`. In this case, the context will already be created (under the name `sc`) and a `ValueError` exception is raised when we try to create a second context. The exception has not effect so we simply catch it and display a warning message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an RDD from text files representing the logs of a supercomputer named Colosse 2014. This data is in the folder `/project/datasets/colosse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabevents = sc.textFile('/project/datasets/colosse/*.ssv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lines of our log file looks like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "00:00:08 1388552403:2422699 job 10563398 JOBEND 1 8 shm 54000 Completed short 1388519145 1388541168 1388541168 1388552397 zcv-890-aa\n",
    "00:00:15 1388552413:2422700 job 10563514 JOBSTART 1 8 shm 54000 Idle short 1388519476 1388552413 1388552413 1388552413 zcv-890-aa\n",
    "00:04:53 1388552691:2422701 job 10563883 JOBEND 3 24 mpourbag 1800 Completed short 1388551677 1388552392 1388552392 1388552670 bem-651-ac\n",
    "00:04:58 1388552698:2422702 job 10563887 JOBSUBMIT 3 24 mpourbag 1800 Idle short 1388552698 0 0 1388552698 bem-651-ac\n",
    "00:06:20 1388552780:2422703 node r101-n75 NODEUP r101-n75 STATE=Idle PARTITION=torque AMEMORY=24155 ASWAP=21803 CDISK=1 CMEMORY=24155 CPROC=8 CSWAP=24155 RACK=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a tabular file, where each line is a single event and the columns represent\n",
    "1. Event time;\n",
    "2. Event epoch;\n",
    "3. Event type;\n",
    "\n",
    "If the event is related to a job, the rest of the columns will be:  \n",
    "4. Job id;\n",
    "5. Job event : `{JOBSUBMIT, JOBSTART, JOBEND}`;\n",
    "6. Number of nodes;\n",
    "7. Number of cores;\n",
    "8. Username;\n",
    "9. Wallclock limit;\n",
    "10. Status;\n",
    "11. Queue;\n",
    "12. Submit time;\n",
    "13. Dispatch time;\n",
    "14. Start time;\n",
    "15. End time;\n",
    "16. Project id.\n",
    "\n",
    "We can look at a few entries with the RDD's method `take` to get the first `K` elements of the dataset. Here, `K = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first10 = moabevents.take(10)\n",
    "print(first10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since `take` returns a list, we can iterate on the result and print each line from the file on a separate line on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "for item in first10:\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting Help\n",
    "\n",
    "At any moment, you can get help on a Python object using the `help()` function. For example, if we want to know more aboud the RDD's `take()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(moabevents.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Action on a Dataset\n",
    "\n",
    "The `take()` method is one among multiple available *actions* we can apply on an RDD. An exhaustive list of actions is available at the following URL:\n",
    "https://spark.apache.org/docs/latest/programming-guide.html#actions\n",
    "\n",
    "In case we would not want to leave the notebook tab, we can call `help()` directly on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(moabevents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the available action, lets take the method `count()`. What does it return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabevents.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action apply on an RDD leads to the creation of one or many task and the production of a result. Every task executed in the same app can be visualized in the Spark's dashboard. In this interface, we can track the progress of a task, and check different performance measures on the task, for example its duration and cache statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the 10 first elements of our dataset that we retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that the RDD is composed of each line of input text files, but that is not possible to access to individual column. **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first1 = moabevents.first()\n",
    "first1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action `first()` as its name states, return the first entry of the dataset. We see that each entry is a single string. We will need to transform that first RDD in a second in order to divide each string in a list. To do this, we will use the method `split()` of Python string object.\n",
    "\n",
    "We first test the function on the dataset first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str.split(first1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to apply this transformation to every RDD's entry. The RDD's method `map(func)` returns a new RDD formed by processing each element of the source with a function *func*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabevents_tab = moabevents.map(str.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of this transformation is *lazy*. Spark does not compute anything as long as a result is not requested by an action. To convince yourself, execute the preceding cell, then visit the Spark dashboard. You should see that no job have been added to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we expect to operate frequently on the same dataset, it can be useful to tell Spark to keep it in memory.\n",
    "\n",
    "To do so, we use the `cache()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabevents_tab.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDDs stored in memory are displayed in the **Storage** section of Spark web interface. Note that datasets are not loaded in memory until an action is called on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To free memory used by cached RDD that we no longer need, we need to call the `unpersist()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabevents_tab.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtering a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have an RDD that is easier to manipulate, we can start the analysis. First, we take interest in events that are related to job. The following line filter the last RDD we created, keeping only the entries that are related to job.\n",
    "\n",
    "Try to answer the following quiz before executing the cell:  \n",
    "* What sort of argument takes the `filter()` method?\n",
    "* What type is `x`?\n",
    "* Is filter an action or a transformation?\n",
    "* What does `filter()` return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabjobs = moabevents_tab.filter(lambda x: x[2] == \"job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "**Write the code to cache the new RDD in the following cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now count the number of job events. The macro notebook `%time` will indicate how long it took Spark to count the number of entries in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time moabjobs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we told Spark to keep the dataset in memory, the time required to count the number of job events should be shorter for the second execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time moabjobs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we committed an action on a cached RDD, it should now figured in the **Storage** section of our app's dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reduction Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in producing a bar chart of the total requested walltime per project in our dataset. In order to do this, we will need to aggregate the requested walltime for each project. This type of operation is called a *reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Filtering Unrelated Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requested wallclock time limit will show up in different events related to the same job. Some of theses events are: \n",
    "```\n",
    "JOBSTART\n",
    "JOBEND\n",
    "JOBSUBMIT\n",
    "```\n",
    "\n",
    "#### Exercise 2\n",
    "**Design a filter to only keep the entries related to job submission.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moabjobends = moabjobs.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Transforming in Key-Value Pairs\n",
    "\n",
    "We now need to transform our dataset to only keep the project and the requested walltime. Furthermore, the requested walltime field is a string, we therefore use `int()` to convert it to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_walltime = moabjobends.map(lambda entry: (entry[-1], int(entry[8])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "**Take a look at the first 5 elements of the new RDD to confirm the transformation is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_walltime.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides functions to work with key-value pairs. In our new dataset, the key is the project id and the wallclock time limit is the value. The `key()` method of an RDD returns a new RDD composed only of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_walltime.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `values()` method is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_walltime.values().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Aggregating the Results by Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the total requested walltime for each project. In order to do this, we use the `reduceByKey()` method. As its name states, this function expect the RDD to be composed of key-value pairs.\n",
    "\n",
    "When called on a dataset of $(K, V)$ pairs, `reduceByKey()` returns a dataset of $(K, V)$ pairs where the values for each key are aggregated using the given reduce function *func*, which must be of type $(V,V) \\rightarrow V$.\n",
    "\n",
    "Since we want the total requested walltime per project, our aggregating function will be the addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_limits = project_walltime.reduceByKey(lambda x, y: x + y).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()` is a transformation, therefore the result is a new RDD. To visualize the entire content of the latter, we can use the `collect()` action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_limits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Finding the Most Processing Hungry Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to determine the 5 projects that requested the most processing walltime in 2014. In order to do this, multiple solutions possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1. Sort locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = sorted(agg_limits.collect(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2 Ask Spark to sort the dataset using the `sortBy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = agg_limits.sortBy(keyfunc=lambda x: x[1], ascending=False).take(5)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3 Ask Spark for the top 5 using the `top()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = agg_limits.top(5, lambda x: x[1])\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "**Can you think of another method to get the same result?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "agg_walltime.takeOrdered(5, key=lambda x: -x[1])"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Bar Chart\n",
    "\n",
    "Since we are in an interactive notebook, we can use Python plotting library `plotly` to plot our bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode() # run at the start of every ipython notebook to use plotly.offline\n",
    "                     # this injects the plotly.js source files into the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally collect our data. We first sort the data in descending order of walltime limt. We then retrieve the keys of our RDD corresponding to the project ids. Finally, we collect the values which correspond to the total walltime limit per project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_agg_limits = agg_limits.sortBy(keyfunc=lambda x: x[1], ascending=False)\n",
    "projects = sorted_agg_limits.keys().collect()\n",
    "times = sorted_agg_limits.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Bar(\n",
    "        x=projects,\n",
    "        y=times,\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Total requested walltime per project in 2014\",\n",
    "    xaxis=dict(\n",
    "        title='Project ID',\n",
    "        tickangle=-45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Total requested walltime',\n",
    "    ),    \n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Mini-Project\n",
    "\n",
    "## Are users good at predicting their jobs' walltime?\n",
    "\n",
    "We would now like to know if the users are good at predicting the walltime of their jobs. For this, you will first need to compute the effective walltime used for each project. Then, create a new chart where we can see the used walltime over the requested walltime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Moab provides for each job the start and end times, these timestamps are sometime innacurate or null. To avoid this, we will first compute ourselves the beginning and ending timestamp for each job based on the timestamp of the event log. Then, we will compute how long was each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering to keep only relevant events\n",
    "\n",
    "We will only take interest in job event that are either of the name `JOBSTART` or `JOBEND`. \n",
    "\n",
    "**Create a new RDD based on `moabjobs` which should only contain events named `JOBSTART` or `JOBEND`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating time and date\n",
    "\n",
    "Python provides the module  `datetime` to manipulate date and time and parse timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `fromtimestamp` can be used to transform an integer representing a Unix timestamp into a datetime object. Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.fromtimestamp(1388552691)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datetime` objects can be subtracted to get a `timedelta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = datetime.fromtimestamp(1388552691) - datetime.fromtimestamp(1388552000)\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timedelta` objects have a seconds attribute to retrieve the value of the delta in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta.seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the previous RDD so the new one will have the following structure:**\n",
    "- key: (job id, account id)\n",
    "- value: a dictionary with one entry\n",
    "    - key: event name (i.e: JOBSTART or JOBEND)\n",
    "    - value: timestamp as `datetime`\n",
    "    \n",
    "Fill the missing part in the function `convert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(rec):\n",
    "    job_id = <FILL_IN>\n",
    "    event_name = <FILL_IN>\n",
    "    timestamp = <FILL_IN>\n",
    "    return job_id, {event_name : timestamp}\n",
    "\n",
    "timestamps = <FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating JOBSTARTs and JOBENDs\n",
    "\n",
    "For the same job id, it is possible that multiple events with the same names were logged. We will want next to only keep one `JOBSTART` and one `JOBEND` for each job id. To do so, we have written a small function that, given the previous RDD structure, can merge entries in such a way that we have the very first `JOBSTART` and the very last `JOBEND`.\n",
    "\n",
    "The function is call `merge_jobtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_jobtypes(a, b):\n",
    "    c = {}\n",
    "    if 'JOBSTART' in a or 'JOBSTART' in b:\n",
    "        c['JOBSTART'] = min(a.get('JOBSTART', b.get('JOBSTART')), \n",
    "                            b.get('JOBSTART', a.get('JOBSTART')))\n",
    "    if 'JOBEND' in a or 'JOBEND' in b:\n",
    "        c['JOBEND'] = max(a.get('JOBEND', b.get('JOBEND')), \n",
    "                          b.get('JOBEND', a.get('JOBEND')))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new RDD by applying the correct transformation using `merge_jobtypes` as the transformation function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter the jobs that do not have a JOBSTART AND a JOBEND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute wallclock time per job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the total wallclock time per project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build an RDD that will contain both the processed walltime and the total requested walltime per project.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort the RDD per total processed walltime in descending order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = project_con_pred.keys().collect()\n",
    "times, limits = zip(*project_con_pred.values().collect())\n",
    "\n",
    "data = [\n",
    "    go.Bar(\n",
    "        x=projects,\n",
    "        y=times,\n",
    "        name='processing time'\n",
    "    ),\n",
    "    go.Bar(\n",
    "        x=projects,\n",
    "        y=limits,\n",
    "        name='predicted time'\n",
    "    )    \n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Walltime per project in 2014\",\n",
    "    xaxis=dict(\n",
    "        title='Project ID',\n",
    "        tickangle=-45\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Walltime',\n",
    "    ),\n",
    "#     barmode='stack'\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ending the Analysis\n",
    "\n",
    "Once the analysis is done, we need to tell Spark to free the resources and destroy the context using the `SparkContext`'s method `stop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Recap\n",
    "\n",
    "In this notebook, we used and learned about the following parts of \n",
    "**[Python Spark API](http://spark.apache.org/docs/latest/api/python/)**:\n",
    "1. Import Spark Python module: \n",
    "**[`import pyspark`](http://spark.apache.org/docs/latest/api/python/pyspark.html)**\n",
    "2. Create a SparkContext:\n",
    "**[`pyspark.SparkContext()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext)**\n",
    "2. Create an RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "3. Take a the first *num* elements from an RDD: \n",
    "**[`Rdd.take(num)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take)**\n",
    "3. Count the number of elements in an RDD: \n",
    "**[`Rdd.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count)**\n",
    "4. Retrieve the first element of an RDD: \n",
    "**[`RDD.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first)**\n",
    "5. Apply a transformation on each element of an RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "4. Cache an RDD:\n",
    "**[`RDD.cache()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache)**\n",
    "5. Remove an RDD from memory: \n",
    "**[`RDD.unpersist()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.unpersist)**\n",
    "5. Filter an RDD:\n",
    "**[`RDD.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter)**\n",
    "6. Merge the values for each keys: \n",
    "**[`RDD.reduceByKey(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)**\n",
    "7. Get all elements of an RDD: \n",
    "**[`RDD.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect)**\n",
    "8. Sort the elements of an RDD:\n",
    "**[`RDD.sortBy(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy)**\n",
    "9. Get the top $N$ elements from an RDD:\n",
    "**[`RDD.top(N)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top)**\n",
    "9. Join two RDDs by key:\n",
    "**[`RDD.join(RDD)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.join)**\n",
    "10. End the SparkContext:\n",
    "**[`SparkContext.stop()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.stop)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 11. References\n",
    "\n",
    "* [Berkeley AmpCamp 5 - Data Exploration Using Spark](http://ampcamp.berkeley.edu/5/exercises/data-exploration-using-spark.html)\n",
    "* [edX - Introduction to Big Data with Apache Spark](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)\n",
    "* [edX - Introduction to Big Data with Apache Spark (Github repo)](https://github.com/spark-mooc/mooc-setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
