{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Practice 2 : Working with Key-Value Pairs\n",
    "\n",
    "1. [Counting Words](#1.-Counting-Words)\n",
    "1. [Recap](#Recap)\n",
    "1. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Counting Words\n",
    "\n",
    "*This exercise material is taken from [edX - Scalable Machine Learning Lab2](https://github.com/spark-mooc/mooc-setup/blob/master/ML_lab2_word_count_student.ipynb).*\n",
    "\n",
    "The volume of unstructured text in existence is growing dramatically, and Spark is an excellent tool for analyzing this type of data. In this lab, we will write code that calculates the most common words in the Complete Works of William Shakespeare retrieved from Project Gutenberg.\n",
    "\n",
    "### 1.1 Capitalization and punctuation\n",
    "Real world files are more complicated than the data we have been using sor far. Some of the issues we have to address are:\n",
    "\n",
    "- Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "- All punctuation should be removed.\n",
    "- Any leading or trailing spaces on a line should be removed.\n",
    "\n",
    "Define the function `remove_punct` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces. Use the Python `re` module to remove any text that is not a letter, number, or space. Reading help(re.sub) might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punct(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.<FILL IN>\n",
    "\n",
    "print(remove_punct('Hi, you!'))\n",
    "print(remove_punct(' No under_score!'))\n",
    "print(remove_punct(' *      Remove punctuation then spaces  * '))\n",
    "print(remove_punct(\" The Elephant's (4 cats). \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Load from a text file\n",
    "\n",
    "Create a new RDD from the text files in Shakespeare's comedies folder. The filename and the path are already configured in `filename`. \n",
    "\n",
    "Once the RDD is created, apply the `remove_punct` transformation and check the first 10 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "basedir = '/scratch/formation/spark/data'\n",
    "inputpath = os.path.join('shakespeare', 'comedies', '*')\n",
    "filename = os.path.join(basedir, inputpath)\n",
    "\n",
    "shakespeareComedies = sc.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Words from lines \n",
    "\n",
    "Before we can count the words' frequency, we have to address two issues with the format of the RDD:\n",
    "\n",
    "- The first issue is that that we need to split each line by its spaces.\n",
    "- The second issue is we need to filter out empty lines.\n",
    "\n",
    "Apply a transformation that will split each element of the RDD.\n",
    "\n",
    "Words can be divided by other characters than simply space, for example tabs (`\\t`). Make sure you cover every case when splitting the lines. Python function `str.split` only covers the case where we want to split the words using a single separating character. If we want multiple characters, we need to look at [`re.split`](https://docs.python.org/3/library/re.html#re.split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3 Remove empty elements\n",
    "\n",
    "The next step is to filter out the empty elements. Remove all entries where the word is ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4 Count the words \n",
    "\n",
    "We now have an RDD that is only words. The next step is to transform this RDD in a key-value pair RDD and count the word. \n",
    "\n",
    "Once this is done, return the 10 least common word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.5 Halt the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Recap\n",
    "\n",
    "In this notebook, we used and learned about the following parts of \n",
    "**[Python Spark API](http://spark.apache.org/docs/latest/api/python/)**:\n",
    "2. Create an RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "5. Apply a transformation on each element of an RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "5. Apply a transformation on each element of an RDD  then flatten the results.:\n",
    "**[`RDD.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap)**\n",
    "5. Filter an RDD:\n",
    "**[`RDD.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter)**\n",
    "6. Merge the values for each keys: \n",
    "**[`RDD.reduceByKey(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)**\n",
    "7. Get the N elements from a RDD ordered in ascending order: **[`RDD.takeOrdered(N)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* [O'Reilly Learning Spark - Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia](http://shop.oreilly.com/product/0636920028512.do)\n",
    "* [Heather Miller - Parallel Programming and Data Analysis](http://heather.miller.am/teaching/cs212/slides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (CQ-Spark)",
   "language": "python",
   "name": "spark-cq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
