{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2 : Working with Key-Value Pairs\n",
    "\n",
    "1. [Counting Words](#1.-Counting-Words)\n",
    "1. [Recap](#Recap)\n",
    "1. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Counting Words\n",
    "\n",
    "*This exercise material is taken from [edX - Scalable Machine Learning Lab2](https://github.com/spark-mooc/mooc-setup/blob/master/ML_lab2_word_count_student.ipynb).*\n",
    "\n",
    "The volume of unstructured text in existence is growing dramatically, and Spark is an excellent tool for analyzing this type of data. In this lab, we will write code that calculates the most common words in the Complete Works of William Shakespeare retrieved from Project Gutenberg.\n",
    "\n",
    "### 1.1 Capitalization and punctuation\n",
    "Real world files are more complicated than the data we have been using sor far. Some of the issues we have to address are:\n",
    "\n",
    "- Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "- All punctuation should be removed.\n",
    "- Any leading or trailing spaces on a line should be removed.\n",
    "\n",
    "Define the function `remove_punct` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces. Use the Python `re` module to remove any text that is not a letter, number, or space. Reading help(re.sub) might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi  you\n",
      "No under score\n",
      "Remove punctuation then spaces\n",
      "The Elephant s  4 cats\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "def remove_punct(word):\n",
    "    return re.sub(r'[{}‘—’”“]'.format(punctuation), \" \", word).strip()\n",
    "\n",
    "print(remove_punct('Hi, you!'))\n",
    "print(remove_punct(' No under_score!'))\n",
    "print(remove_punct(' *      Remove punctuation then spaces  * '))\n",
    "print(remove_punct(\" The Elephant's (4 cats). \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   111 ***\n"
     ]
    }
   ],
   "source": [
    "print(\"   1   111 ***     \".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1 111 *** '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\s{1,}', ' ', \"   1   111 ***     \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load from a text file\n",
    "\n",
    "Create a new RDD from the text files in Shakespeare's comedies folder. The filename and the path are already configured in `filename`. \n",
    "\n",
    "Once the RDD is created, apply the `remove_punct` transformation and check the first 10 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SparkConf in module pyspark.conf:\n",
      "\n",
      "class SparkConf(builtins.object)\n",
      " |  Configuration for a Spark application. Used to set various Spark\n",
      " |  parameters as key-value pairs.\n",
      " |  \n",
      " |  Most of the time, you would create a SparkConf object with\n",
      " |  C{SparkConf()}, which will load values from C{spark.*} Java system\n",
      " |  properties as well. In this case, any parameters you set directly on\n",
      " |  the C{SparkConf} object take priority over system properties.\n",
      " |  \n",
      " |  For unit tests, you can also call C{SparkConf(false)} to skip\n",
      " |  loading external settings and get the same configuration no matter\n",
      " |  what the system properties are.\n",
      " |  \n",
      " |  All setter methods in this class support chaining. For example,\n",
      " |  you can write C{conf.setMaster(\"local\").setAppName(\"My app\")}.\n",
      " |  \n",
      " |  .. note:: Once a SparkConf object is passed to Spark, it is cloned\n",
      " |      and can no longer be modified by the user.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, loadDefaults=True, _jvm=None, _jconf=None)\n",
      " |      Create a new Spark configuration.\n",
      " |      \n",
      " |      :param loadDefaults: whether to load values from Java system\n",
      " |             properties (True by default)\n",
      " |      :param _jvm: internal parameter used to pass a handle to the\n",
      " |             Java VM; does not need to be set by users\n",
      " |      :param _jconf: Optionally pass in an existing SparkConf handle\n",
      " |             to use its parameters\n",
      " |  \n",
      " |  contains(self, key)\n",
      " |      Does this configuration contain a given key?\n",
      " |  \n",
      " |  get(self, key, defaultValue=None)\n",
      " |      Get the configured value for some key, or return a default otherwise.\n",
      " |  \n",
      " |  getAll(self)\n",
      " |      Get all values as a list of key-value pairs.\n",
      " |  \n",
      " |  set(self, key, value)\n",
      " |      Set a configuration property.\n",
      " |  \n",
      " |  setAll(self, pairs)\n",
      " |      Set multiple parameters, passed as a list of key-value pairs.\n",
      " |      \n",
      " |      :param pairs: list of key-value pairs to set\n",
      " |  \n",
      " |  setAppName(self, value)\n",
      " |      Set application name.\n",
      " |  \n",
      " |  setExecutorEnv(self, key=None, value=None, pairs=None)\n",
      " |      Set an environment variable to be passed to executors.\n",
      " |  \n",
      " |  setIfMissing(self, key, value)\n",
      " |      Set a configuration property, if not already set.\n",
      " |  \n",
      " |  setMaster(self, value)\n",
      " |      Set master URL to connect to.\n",
      " |  \n",
      " |  setSparkHome(self, value)\n",
      " |      Set path where Spark is installed on worker nodes.\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      Returns a printable version of the configuration, as a list of\n",
      " |      key=value pairs, one per line.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyspark.SparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "basedir = '/project/datasets'\n",
    "inputpath = os.path.join('shakespeare', 'comedies', '*')\n",
    "filename = os.path.join(basedir, inputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/project/datasets/shakespeare/comedies/*'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allswellthatendswell  merchantofvenice\t     tempest\r\n",
      "asyoulikeit\t      merrywivesofwindsor    troilusandcressida\r\n",
      "comedyoferrors\t      midsummersnightsdream  twelfthnight\r\n",
      "cymbeline\t      muchadoaboutnothing    twogentlemenofverona\r\n",
      "loveslabourslost      periclesprinceoftyre   winterstale\r\n",
      "measureforemeasure    tamingoftheshrew\r\n"
     ]
    }
   ],
   "source": [
    "! ls /project/datasets/shakespeare/comedies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeareComedies = sc.textFile(filename).map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A MIDSUMMER NIGHT S DREAM',\n",
       " '',\n",
       " '',\n",
       " 'DRAMATIS PERSONAE',\n",
       " '',\n",
       " '',\n",
       " 'THESEUS\\tDuke of Athens',\n",
       " '',\n",
       " 'EGEUS\\tfather to Hermia',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeareComedies.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Words from lines \n",
    "\n",
    "Before we can count the words' frequency, we have to address two issues with the format of the RDD:\n",
    "\n",
    "- The first issue is that that we need to split each line by its spaces.\n",
    "- The second issue is we need to filter out empty lines.\n",
    "\n",
    "Apply a transformation that will split each element of the RDD.\n",
    "\n",
    "Words can be divided by other characters than simply space, for example tabs (`\\t`). Make sure you cover every case when splitting the lines. Python function `str.split` only covers the case where we want to split the words using a single separating character. If we want multiple characters, we need to look at [`re.split`](https://docs.python.org/3/library/re.html#re.split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedies_word = shakespeareComedies.flatMap(lambda word: word.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove empty elements\n",
    "\n",
    "The next step is to filter out the empty elements. Remove all entries where the word is ''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedies_word_only = comedies_word.filter(lambda word: len(word) > 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'This'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool('This')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Count the words \n",
    "\n",
    "We now have an RDD that is only words. The next step is to transform this RDD in a key-value pair RDD and count the word. \n",
    "\n",
    "Once this is done, return the 10 least common word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'MIDSUMMER',\n",
       " 'NIGHT',\n",
       " 'S',\n",
       " 'DREAM',\n",
       " 'DRAMATIS',\n",
       " 'PERSONAE',\n",
       " 'THESEUS',\n",
       " 'Duke',\n",
       " 'of']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedies_word_only.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedies_word_pair = comedies_word_only.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 1), ('MIDSUMMER', 1), ('NIGHT', 1), ('S', 1), ('DREAM', 1)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comedies_word_pair.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = comedies_word_pair.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('to', 7082)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_word_frequency = word_frequency.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_word_frequency['book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zodiacs', 1),\n",
       " ('zephyrs', 1),\n",
       " ('zenith', 1),\n",
       " ('zealous', 2),\n",
       " ('zeal', 10),\n",
       " ('zany', 1),\n",
       " ('zanies', 1),\n",
       " ('youths', 2),\n",
       " ('youthful', 12),\n",
       " ('youth', 149)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Stir', 1),\n",
       " ('funerals', 1),\n",
       " ('prevailment', 1),\n",
       " ('shady', 1),\n",
       " ('prosecute', 1),\n",
       " ('collied', 1),\n",
       " ('transpose', 1),\n",
       " ('Skim', 1),\n",
       " ('bouncing', 1),\n",
       " ('hempen', 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency.top(10, key=lambda pair: -pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method top in module pyspark.rdd:\n",
      "\n",
      "top(num, key=None) method of pyspark.rdd.PipelinedRDD instance\n",
      "    Get the top N elements from an RDD.\n",
      "    \n",
      "    .. note:: This method should only be used if the resulting array is expected\n",
      "        to be small, as all the data is loaded into the driver's memory.\n",
      "    \n",
      "    .. note:: It returns the list sorted in descending order.\n",
      "    \n",
      "    >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      "    [12]\n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      "    [6, 5]\n",
      "    >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      "    [4, 3, 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(word_frequency.top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('a', 100) > ('a', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Serenity' > 'Eleanor'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Halt the SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recap\n",
    "\n",
    "In this notebook, we used and learned about the following parts of \n",
    "**[Python Spark API](http://spark.apache.org/docs/latest/api/python/)**:\n",
    "2. Create an RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "5. Apply a transformation on each element of an RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "5. Apply a transformation on each element of an RDD  then flatten the results.:\n",
    "**[`RDD.flatMap(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.flatMap)**\n",
    "5. Filter an RDD:\n",
    "**[`RDD.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter)**\n",
    "6. Merge the values for each keys: \n",
    "**[`RDD.reduceByKey(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)**\n",
    "7. Get the N elements from a RDD ordered in ascending order: **[`RDD.takeOrdered(N)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.takeOrdered)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [O'Reilly Learning Spark - Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia](http://shop.oreilly.com/product/0636920028512.do)\n",
    "* [Heather Miller - Parallel Programming and Data Analysis](http://heather.miller.am/teaching/cs212/slides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
