{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Algorithms\n",
    "\n",
    "## 1. K-Means Clustering\n",
    "\n",
    "### 1.1 Description\n",
    "\n",
    "As stated on Wikipedia\n",
    "> Given a set of observations $(\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n)$, where each observation is a *d*-dimensional real vector, $k$-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $\\mathbf{S}= \\{S_1, S_2, \\ldots, S_k\\}$ so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center). In other words, its objective is to find:\n",
    ">\n",
    ">$\\begin{equation}\\underset{\\mathbf{S}} {\\operatorname{arg\\,min}}  \\sum_{i=1}^{k} \\sum_{\\mathbf{x} \\in S_i} \\left\\| \\mathbf x - \\boldsymbol\\mu_i \\right\\|^2\\end{equation}$\n",
    ">\n",
    ">where $\\boldsymbol\\mu_i$ is the mean of points in $S_i$.\n",
    "\n",
    "\n",
    "### 1.2 Algorithm\n",
    "\n",
    "1. Initialize cluster centroids $\\boldsymbol\\mu_1, \\boldsymbol\\mu_2, \\ldots, \\boldsymbol\\mu_k \\in \\mathbb{R}^n$ randomly\n",
    "2. Repeat until convergence:  \n",
    "  2.1 For every $i$, set $y_i = \\underset{j}{\\arg\\min} \\|\\mathbf{x}_i - \\boldsymbol\\mu_j\\|^2$  \n",
    "  2.2 For each $j$, set $\\boldsymbol\\mu_j = \\frac{1}{\\sum_i^n1\\{y_i = j\\}}\\sum_i^n1\\{y_i = j\\}\\mathbf{x}_i$\n",
    "\n",
    "### 1.3 Objective\n",
    "\n",
    "Write the k-means algorithm using [Spark's RDD API](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4 Dataset\n",
    "\n",
    "To test our algorithm, we will generate a 2D dataset using [scikit-learn](http://scikit-learn.org/stable/datasets/index.html#sample-generators). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N_SAMPLES = 10**3\n",
    "N_FEATURES= 2\n",
    "N_CENTERS = 4\n",
    "STD = 0.5\n",
    "BOX = (-10.0, 10.0)\n",
    "features, tlabels = make_blobs(n_samples=N_SAMPLES, \n",
    "                               n_features=N_FEATURES, \n",
    "                               centers=N_CENTERS, \n",
    "                               cluster_std=STD, \n",
    "                               center_box=BOX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We then create an RDD by parallelizing the features. Since, we will iterate on multiple time on the data, we will benefit from caching it in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(features).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.5 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_label(point, centroids):\n",
    "    \"\"\"Return the label of the closest centroid.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "def kmeans(rdd, n_centers, n_iter)\n",
    "    # 0. Compute dataset bounding box\n",
    "    # 1. Initialize cluster centroids\n",
    "    # 2. Repeat until convergence\n",
    "    #   2.1 Compute label for each points\n",
    "    #   2.2 Update centroids\n",
    "    # 3. Return the final centroids\n",
    "    <FILL IN>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Test your algorithm on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmeans(<FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.6 Compare with MLlib Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clusters = KMeans.train(rdd, N_CENTERS, maxIterations=10, initializationMode=\"random\")\n",
    "clusters.centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Recommending Friends\n",
    "\n",
    "This exercise draws heavily from Washington University course CS140: Data Programming's [Homework 4: Social networking and recommendation systems](https://courses.cs.washington.edu/courses/cse140/13wi/homework/hw4/homework4.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  2.1 Description\n",
    "\n",
    "Social network websites (Facebook, Twitter, LinkedIn) suggest person you should be connected (or friend) with. How do they do that?\n",
    "\n",
    "A social network can be represented as a graph. Persons are represented as nodes or vertices, and the relationships are represented as edges. An edge between person A and person B means that A considers B a friend, and also B considers A a friend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2 Objective\n",
    "\n",
    "For user X, list some non-friends in order, starting with the best friend recommendation and ending with the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.3 Algorithm\n",
    "\n",
    "If non-friend Y is your friend's friend, then maybe Y should be your friend too. If person Y is the friend of many of your friends, then Y is an even better recommendation. The best friend recommendation is the person with whom you have the largest number of mutual friends. You will implement this heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.4 Dataset\n",
    "\n",
    "In this exercise, you will start from the dataset \"[Social characteristics of the Marvel Universe](http://bioinfo.uib.es/~joemiro/marvel.html)\" which represents a list of relationships between characters in the [Marvel Universe](http://marvel.com/universe/Main_Page). \n",
    "\n",
    "From this dataset you should be able to infer a graph and determine friend recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "               .options(inferschema=False, header=False)\\\n",
    "               .load('/scratch/formation/spark/data/marvel/hero-network.csv')\\\n",
    "               .distinct().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.5 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. References\n",
    "\n",
    "- [edX - Scalable Machine Learning Course](https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x)\n",
    "- [edX - Scalable Machine Learnig Notebboks](https://github.com/spark-mooc/mooc-setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (CQ-Spark)",
   "language": "python",
   "name": "spark-cq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
